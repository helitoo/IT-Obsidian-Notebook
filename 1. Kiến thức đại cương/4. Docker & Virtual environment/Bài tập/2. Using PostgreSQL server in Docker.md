
# NY Taxi Dataset and Data Ingestion

Chúng ta sẽ sử dụng dataset này: [Yellow taxi trip records CSV file for January 2021](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz). Bạn không cần phải giải nén nó mà có thể đưa nó thẳng vào `ny_taxi_cli/`.

Đồng thời tải thêm các package này:
```sh
uv add pandas sqlalchemy psycopg2-binary
```

**1 - Kiểm tra dataset**:

`notebook.ipynb`
```python
import pandas as pd

# Read a sample of the data
prefix = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/'

dtype = {
    "VendorID": "Int64",
    "passenger_count": "Int64",
    "trip_distance": "float64",
    "RatecodeID": "Int64",
    "store_and_fwd_flag": "string",
    "PULocationID": "Int64",
    "DOLocationID": "Int64",
    "payment_type": "Int64",
    "fare_amount": "float64",
    "extra": "float64",
    "mta_tax": "float64",
    "tip_amount": "float64",
    "tolls_amount": "float64",
    "improvement_surcharge": "float64",
    "total_amount": "float64",
    "congestion_surcharge": "float64"
}

parse_dates = [
    "tpep_pickup_datetime",
    "tpep_dropoff_datetime"
]

df = pd.read_csv(
    prefix + 'yellow_tripdata_2021-01.csv.gz',
    nrows=100,
    dtype=dtype,
    parse_dates=parse_dates
)

# Display first rows
df.head()
```

**2 - Kết nối với database**:

Kết nối với database:
`notebook.ipynb`
```python
from sqlalchemy import create_engine
engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')

engine
```

```sh
Engine(postgresql://root:***@localhost:5432/ny_taxi)
```

**3 - Đưa dữ liệu vào database**:

Do dữ liệu khá nhiều, ta sẽ insert theo từng đợt (batch):

`notebook.ipynb`
```python
df_iter = pd.read_csv(
    prefix + 'yellow_tripdata_2021-01.csv.gz',
    dtype=dtype,
    parse_dates=parse_dates,
    iterator=True,
    chunksize=100000
)

first = True

for df_chunk in df_iter:
	
    if first:
        # Create table schema (n=0 -> no data)
        df_chunk.head(0).to_sql(
            name="yellow_taxi_data",
            con=engine,
            if_exists="replace"
        )
        first = False
        print("Table created")
	
    # Insert chunk
    df_chunk.to_sql(
        name="yellow_taxi_data",
        con=engine,
        if_exists="append"
    )
	
    print("Inserted: ", len(df_chunk))
```

Bạn sẽ thấy dữ liệu được đưa vào database theo từng đợt (*chunk / batch*, mỗi đợt 100000 record).

```sh
Table created
Inserted:  100000
Inserted:  100000
Inserted:  100000
Inserted:  100000
Inserted:  100000
Inserted:  100000
Inserted:  100000
Inserted:  100000
Inserted:  100000
Inserted:  100000
Inserted:  100000
Inserted:  100000
Inserted:  100000
Inserted:  69765
```

Để trực quan hóa quá trình insert dữ liệu, có thể sử dụng package `tdqm`. Xem [tdqm docs](https://tqdm.github.io/).

# Creating the Data Ingestion Script

Sử dụng `nbconvert` để chuyển notebook thành Python script.

```sh
uv run jupyter nbconvert --to=script notebook.ipynb
mv notebook.py ingest_data.py
```

Kết quả, thu được file `ingest_data.py` (đổi tên từ `notebook.py`).

Sử dụng `click` để định nghĩa CLI cho `ingest_data.py`:

`ingest_data.py`:
```python
import click

@click.command()
@click.option('--user', default='root', help='PostgreSQL user')
@click.option('--password', default='root', help='PostgreSQL password')
@click.option('--host', default='localhost', help='PostgreSQL host')
@click.option('--port', default=5432, type=int, help='PostgreSQL port')
@click.option('--db', default='ny_taxi', help='PostgreSQL database name')
@click.option('--table', default='yellow_taxi_data', help='Target table name')
def ingest_data(user, password, host, port, db, table):
    # Ingestion logic here
    # ...
    pass
```

Kể từ lúc này, `ingest_data.py` có thể được thực thi nhanh qua Bash prompt:
```sh
uv run python ingest_data.py \
  --user=root \
  --password=root \
  --host=localhost \
  --port=5432 \
  --db=ny_taxi \
  --table=yellow_taxi_trips
```


