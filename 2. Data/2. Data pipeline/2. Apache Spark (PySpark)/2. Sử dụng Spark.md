
# SparkContext & SparkSession

**`SparkContext`**:
- Dùng từ Spark 1.x.
- Có chức năng kết nối Spark cluster và thực thi các tác vụ lên cluster như *tạo RDD, xử lý dữ liệu,...*

**`SparkSession`**:
- Dùng từ Spark 2.x.
- Cung cấp các cơ chế để giao tiếp với Spark (*Unified entry point*) thông qua `DataFrame`, `Dataset` dưới nhiều API.
- Kết hợp và mở rộng các chức năng của `SparkContext`, `SQLContext`, `HiveContext`, `StreamingContext`.

**Tạo `SparkContext`**:
```python
from pyspark import SparkContext

sc = SparkContext(appName='MyApp')
```

**Tạo `SparkSession`**:
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
	.appName('MyApp') \
	.getOrCreate()
	
# Get SparkContext from SparkSession
# sc = spark.sparkContext
```

Ngoài `.appName('urName')`, thuộc tính `.build` cung cấp một số phương thức cấu hình như sau:
- `.config('spark.excutor.memory', '2g')`: Cấp cho Spark application 2GB bộ nhớ.
- `.config('spark.sql.shuffle.partitions', '4')`: Số partition của RDD.

Để tìm thêm các cấu hình, hãy mở GUI của `spark` sau khi khởi tạo nó, chỉ bằng cách gọi `spark`.


Sau khi đóng notebook, bạn chỉ đơn giản là đóng GUI để sử dụng, Spark application vẫn đang ngầm hoạt động.
**Đóng Spark application**:
```python
spark.stop()
# sc.stop()
```

# Resilient Distributed Datasets (RDD)

## Kiến trúc RDD

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTqnzeGWVV1K3ax8w3z4u3-WbMpBL6DeiDTCA&s)

RDD là cấu trúc dữ liệu nền tảng của Apache Spark, cho phép xử lý dữ liệu phân tán trên các cụm máy tính một cách linh hoạt và hiệu quả.

**Đặc điểm của RDD**:
- **Immutable**: Mỗi transaction trên RDD đều tạo ra các RDD mới chứ không thể thay đổi RDD cũ.
- **Resilient / Fault-tolerance**: Do các transantion đều được lưu lại trên RDD, khi dữ liệu mất đi, Spark có thể truy vết để khôi phục dữ liệu (*lineage graph*).
- **Distributed**: Dữ liệu trên RDD được chia ra thành các phần nhỏ hơn, độc lập với nhau (*partition*). Các phần này được phân phối các nút (*machine*) khác nhau trong cụm (*cluster*) -> Cho phép xử lý song song (*parallel*).
- **Lazied avaluated**: Các tác vụ chỉ được thực thi khi cần thiết.

**Các thao tác (operation) trên RDD**:
- **Transformation**:
	- Áp dụng một phép tính (*computation*) hoặc thao tác (*manipulation*) lên dữ liệu để biến đổi nó và tạo ra RDD mới.
	- Lazied avaluated, Resilient.
	- VD: `map`, `filter`, `reduceByKey`, `sortBy`, `join`,...
- **Action**:
	- Áp dụng một phép tính (*computation*) hoặc thao tác (*manipulation*) lên dữ liệu để thu thập một thông tin gì đó, không tạo ra RDD mới.
	- Eager avaluated.
	- VD: `count`, `collect`, `first`, `take`, `save`, `forEach`,...

**Tài liệu tham khảo**:
- Các phương thức Transformations: [Apache Spark / RDD programming guide / Transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations).
- Các phương thức Actions: [Apache Spark / RDD programming guide / Actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions).
- Cú pháp cụ thể: [Apache Spark / Python API reference / pyspark.RDD](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD).

## Sử dụng RDD

Chỉ có `SparkContext` mới có thể khởi tạo trực tiếp RDD thông qua phương thức `.parallelize()` hoặc `.textFile()`.

```python
numbers = [1, 2, 3, 4, 5] # Or any kind of data types
rdd = spark.SparkContext.parallelize(numbers)
```

# DataFrame

## Kiến trúc DataFrame

**DataFrame** là cấu trúc dữ liệu dảng bảng, bao gồm các cột và hàng.

**Ưu điểm của DataFrame so với RDD**:
- **Optimized execution**: Các thông tin về schema của DataFrame cho phép Spark thực thi các tác vụ nhanh hơn so với RDD.
- **Ease of use**: DataFrame cung cấp các phương thức tương tự SQL, giúp nó dễ dàng tiếp cận hơn đối với người dùng bắt đầu từ SQL.
- **Integration with ecosystem**: DataFrame hỗ trợ nhiều phương thức đến từ *Spark SQL, MLlib, GraphX,...*.
- **Interoperability**: Dễ dàng chuyển đổi sang các kiểu dữ liệu khác.

## Sử dụng DataFrame

Chỉ có `SparkSession` mới có thể khởi tạo trực tiếp RDD thông qua phương thức `.createDataFrame()` hoặc các phương thức đọc dữ liệu.

Các phương thức của Spark DataFrame cũng gần giống như Pandas DataFrame.

**Tài liệu tham khảo**:
- Các phương thức chuyên dụng của DataFrame: [Apache Spark / Python API reference / DataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html).
- Các phương thức xây dựng dữ liệu của DataFrame: [Apache Spark / Python API reference / Pyspark Session](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html).
- Phương thức truy vấn bằng cú pháp SQL: [Apache Spark / Python API reference / pyspark.sql](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.sql.html#pyspark.sql.SparkSession.sql).
- Các phương thức xây dựng schema của DataFrame: [Apache Spark / Python API reference / Data types](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html).

VD: **Khởi tạo DataFrame**:

```python
df = spark.createDataFrame([
    {"deptId": 1, "age": 40, "name": "Hyukjin Kwon", "gender": "M", "salary": 50},
    {"deptId": 1, "age": 50, "name": "Takuya Ueshin", "gender": "M", "salary": 100},
    {"deptId": 2, "age": 60, "name": "Xinrong Meng", "gender": "F", "salary": 150},
    {"deptId": 3, "age": 20, "name": "Haejoon Lee", "gender": "M", "salary": 200}
])
```

```python
df = spark.read.csv('./data/products.csv', header=True)
```

VD: **Khởi tạo schema và đọc csv từ schema**:

```python
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

schema = StructType([
	StructField(name='id', dataType=IntegerType(), nullable=False),
	StructField(name='name', dataType=StringType(), nullable=False),
	StructField(name='price', dataType=DoubleType(), nullable=False),
])

df = spark.read.csv('./data/products.csv', header=True, schema=schema)
```

Ngoài ra, `.read.csv` còn một số tham số là:
- `inferSchema=True`: Tự động suy diễn kiểu dữ liệu của cột.
















