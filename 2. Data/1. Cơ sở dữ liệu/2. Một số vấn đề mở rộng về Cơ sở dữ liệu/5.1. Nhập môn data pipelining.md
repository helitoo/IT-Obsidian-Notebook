
![](https://blog.bismart.com/hubfs/Imported_Blog_Media/20190604_imagen2-Sep-26-2023-09-18-41-0016-AM.jpg)

Dữ liệu của warehouse đến từ *nhiều nguồn khác nhau*, có thể là nhiều database, mỗi database có thể theo một mô hình dữ liệu riêng, một hệ quản trị riêng,... Khi dữ liệu từ các nguồn đó được chuyển vào warehouse có thể xảy ra mâu thuẫn, vì:
	1. Định dạng dữ liệu giữa các nguồn không giống nhau.
	2. Dữ liệu không sạch, không chuẩn hóa.
	3. Dữ liệu kém chất lượng, dữ liệu bị thiếu, bị nhiễu.

Vì thế cần một quy trình chuyển đổi các dữ liệu từ nhiều nguồn vào một nguồn đích là **ETL**..

# Tổng quan về data pipeline

Cả ETL và ELT đều gồm 3 bước:

1. **E - Extract**: Lấy dữ liệu ra khỏi các nguồn dữ liệu.
2. **T - Transform**: Biến đổi dữ liệu từ định dạng nguồn sang định dạng đích.
3. **L - Load**: Đưa dữ liệu vào

**ETL**:
- Là quy trình thực thi các bước theo thứ tự **E -> T -> L**.
- Trong quá trình vận hành, dữ liệu đã bị biến đổi rồi mới lưu lại tại đích, dễ thất lạc dữ liệu, nên tính an toàn không cao.
- Phù hợp với đích là các database có cấu trúc, quan hệ chặt chẽ và data warehouse.

**ELT**:
- Là quy trình thực thi các bước theo thứ tự **E -> L -> T**. Tức là nó lưu dữ liệu từ nguồn vào đích trước rồi biến đổi sau.
- Do dữ liệu nguồn đã được lưu trữ tại đích rồi, nên dữ liệu được đảm bảo an toàn, không bị thất thoát. Tuy nhiên, quy trình. Nhưng trên thực tế, triển khai ELT khó hơn ETL nhiều lần, và cũng dễ xảy ra các vấn đề về bảo mật dữ liệu khi dữ liệu không trải qua xử lý mà đã được lưu vào đích.
- Phù hợp với đích là các database No-SQL như Hadoop, Data Appliance hoặc Cloud Installation.

Quy trình mà dữ liệu từ nguồn, thông qua ETL/ELT và đến đích gọi là **data pipeline**.

Độ phức tạp của pipeline:
>[!QUOTE]
>Trong quá trình phát triển data warehouse, khoảng **60-80%** thời gian và công sức được dành ra cho ETL.
>*_Microsoft*

Tầm quan trọng của pipeline:
>[!QUOTE]
>**"Garbage in, Garbage out (GIGO)"**: Chất lượng của 1 hệ thống chính là chất lượng của dữ liệu lưu trong hệ thống đó.

# Một số kỹ thuật dùng trong data pipeline

1. **Phân loại dữ liệu (Data profiling)**: Kiểm tra chất lượng dữ liệu, khám phá cấu trúc dữ liệu và tìm hiểu mối liên hệ giữa các bảng, cột và giá trị.
2. **Làm sạch dữ liệu (Data cleaning)**: Loại bỏ giá trị bị lỗi, bị thiếu hoặc không hợp lệ.
3. **Định dạng dữ liệu (Data formating)**: Định dạng lại để phù hợp với yêu cầu của hệ thống mục tiêu.
4. **Tự động hóa (Automation)**: Trong một số trường hợp, quy trình ETL cần phải được thực hiện định kỳ, ví dụ hàng ngày hoặc hàng tuần. Trong trường hợp này, việc tự động hóa quá trình ETL là rất quan trọng, giúp tiết kiệm thời gian và công sức.
5. **Xử lý dữ liệu lớn (Big data processing)**: Phân tách dữ liệu (data partitioning), song song hóa (parallelism), Hadoop, Spark,...
6. **Xử lý thời gian thực (Real-time processing)**: Trong một số ngữ cảnh, dữ liệu cần được xử lý ngay lập tức sau khi nó được tạo ra. Cần áp dụng các công nghệ như Apache Kafka, Amazon Kinesis, Google Cloud Pub/Sub,...
7. **Xử lý song song (Parallel)**: Kích hoạt nhiều ETL hoạt động cùng lúc thay vì khởi chạy tuần tự, tiết kiệm thời gian, chi phí, tận dụng tối đa sức mạnh máy tính.
8. **Ứng dụng AI**: AI có vai trò cải thiện chất lượng dữ liệu, phân loại dữ liệu, và tự động hóa quy trình ETL.

# Quy trình ETL trong data warehouse

## Extract

Có một vài kỹ thuật sau:
1. **OCR (Optical character recognition)**: Trích xuất văn bản từ tài liệu ảnh, tài liệu `pdf`,...
2. **ACD (Analog-to-Digital Converter)**: Trích xuất văn bản từ các tín hiệu số và âm thanh.
3. **CCD (Charge-Coupled device)**: Chụp và số hóa hình ảnh.
4. **API (Application programming interface)**: Lấy dữ liệu thông qua các API được cung cấp bởi nguồn dữ liệu đó.
5. **Web scraping**: Cào dữ liệu trên các trang web về máy tính.
6. **SQL (Structured query language)**: Truy vấn dữ liệu từ các database.
7. ...

## Stage data

**Staging table** là các bảng tạm được tạo ra để lưu trữ dữ liệu trong quá trình ETL. Việc sử dụng bảng tạm là cần thiết vì:
1. **Giảm áp lực** lên warehouse, tránh tải cùng lúc một lượng khổng lồ dữ liệu. Nguyên lý này cũng giống với server và cache.
2. Qua đó cũng có thể **tăng hiệu năng** ETL.
3. Nếu trong quá trình ETL xảy ra sự cố, các bảng tạm đóng vai trò **backup dữ liệu**, không cần tải lại dữ liệu từ các nguồn từ đầu nữa.

Staging table thường là các bảng nằm trong warehouse, có cấu trúc giống cấu trúc bảng nguồn.

Nhược điểm của stage table là cần sao chép dữ liệu từ nguồn vào tage table trước tiên.
Có một phương pháp khác để xây dựng stage data mà không cần stage table là **Ảo hóa dữ liệu (data virtualization)**. Ảo hóa là mở và đọc dữ liệu trực tiếp từ nguồn thay vì sao chép, di chuyển dữ liệu qua lại giữa các nơi lưu trữ.

VD: SQL Server:
```sql
SELECT c.Name, s.Amount, w.City
FROM CustomerDB.dbo.CUSTOMERS c
JOIN SalesDB.dbo.SALES s
	ON c.ID = s.CustomerID
JOIN ExternalWarehouse.WEATHERDATA w
	ON c.City = w.City
```
Thoạt nhìn thì trông như cả `CUSTOMERS`, `SALES`, `WEATHERDATA` đều nằm chung một database, nhưng thực chất chúng nằm ở 3 nơi khác nhau:
1. `CustomerDB.dbo.CUSTOMERS`: Bảng từ database `CustomerDB`.
2. `SalesDB.dbo.SALES`: Bảng từ  database `SalesDB`.
3. `ExternalWarehouse.WEATHERDATA`: Dữ liệu ngoại lai, không nằm trong database nào. Có thể nó có định dạng `.csv`.

## Transform

Có một vài thao tác như sau:
1. **Kết hợp dữ liệu**:
	1. Kết hợp dựa trên **khóa chính - khóa ngoại** (`JOIN`).
	2. Kết hợp dựa trên **các cột giống nhau** (`UNION`).
2. **Tính toán dữ liệu**:
	1. Tăng số chiều. VD: Từ 1 thuộc tính `DayMonthYear` tách ra thành 3 thuộc tính `Day`, `Month`, `Year` (`SUBSTRING`). Có thể thuộc 3 bảng khác nhau.
	2. Giảm số chiều. VD: Từ 3 thuộc tính `Day`, `Month`, `Year` ghép lại thành 1 thuộc tính duy nhất `DayMonYear` (`CONCAT`).
3. **Chuyển đổi kiểu dữ liệu**.
4. **Chỉnh sửa nội dung**.
5. **Ghi nhận lịch sử thay đổi**.
6. ...

## Load

Thứ tự load: **Load các table được tham chiếu trước, load các table tham chiếu sau**.
- Cũng có nghĩa là load dimension table trước, fact table sau.
- Nếu trong các dimension table có cấu trúc đặc biệt thì cũng cần suy nghĩ thứ tự load cho hợp lý, tuân theo quy tắc trên.
- Khi cập nhật fact table:
	- Nếu xảy ra **mâu thuẫn** giữa dữ liệu mới và dữ liệu sẵn có trong fact table. Cần thêm dữ liệu dựa trên loại SCD mà fact table đó áp dụng.
	- Nếu xuất hiện dữ liệu **không thể tham chiếu** đến bất kỳ dimension nào, cân nhắc gắn nhãn `unknown` hoặc tạo thêm dimension mới, thuộc tính mới.

>[!IMPORTANT]
>Trong một số trường hợp **bất đắc dĩ**, cần phải xóa một phần hoặc xóa hết toàn bộ fact table mà chèn lại dữ liệu từ đầu.

Một số chiến lược tải dữ liệu:
1. **Full loading**: Xóa hết dữ liệu và đưa toàn bộ dữ liệu mới vào warehouse, thích hợp với những dữ liệu không thay đổi thường xuyên.
2. **Incremental loading**: Chỉ đưa vào warehouse có dữ liệu mới nhất.
3. **Scheduled loading / Batch processing**: Dữ liệu được đưa vào warehouse theo từng đợt riêng lẻ, được lên kế hoạch trước, thích hợp với các hệ thống có tốc độ xử lý chậm, dữ liệu không thay đổi thường xuyên.
4. **Real-time loading / Stream processing**: Dữ liệu được đưa vào warehouse ngay khi nó được sinh ra, phù hợp với các loại dữ liệu thay đổi liên tục, hệ thống phải có tốc độ xử lý cao.
5. **On-demand loading**: Dữ liệu được đưa vào warehouse lúc nào tùy theo nhu cầu người dùng, giúp tiết kiệm chi phí do ít lần tải nhưng đòi hỏi hệ thống phải có tốc độ xử lý cao để đáp ứng người dùng.

## Logging

**Logging** là ghi lại lịch sử các thao tác trong quá trình ETL.

Logging có thể là một bảng độc lập, hoặc gắn vào stage table, hoặc gắn vào fact table, tùy vào tình hình thực tế.

Bao gồm một số nội dung sau:
1. **ID**: Mã định danh cho mỗi thao tác.
2. Thời điểm bắt đầu, thời điểm kết thúc.
3. Trạng thái (thất bại hay thành công).
4. Sự cố xảy ra ngoài ý muốn.
















