
**Tài liệu tham khảo**:
- https://www.kaggle.com/learn/feature-engineering.
- https://www.kaggle.com/learn/data-cleaning.
- https://www.kaggle.com/learn/intermediate-machine-learning.
- https://inria.github.io/scikit-learn-mooc/predictive_modeling_pipeline/predictive_modeling_module_intro.html.

**Feature engineer (Feature extraction)** là quá trình *chọn lọc, biến đổi* từ liệu thô ban đầu (raw data) thành các đặc trưng (vector).

**Lợi ích**:
- Cải thiện chất lượng model.
- Giảm khối lương dữ liệu training, qua đó tăng hiệu năng.
- Tăng độ tường minh của thuật toán.

# Mutual information (Feature utility metric)

**Mutual information (MI)** là bảng thể hiện sự liên quan giữa 2 feature với nhau.
- **MI = 0**: 2 feature không liên quan gì đến nhau.
- Về lý thuyết, MI không có cận trên. Nhưng thường thì *MI < 2*.

**Entropy** là đại lượng đo mức độ không chắc chắn (độ hỗn loạn thông tin) của một feature, trước khi xét đến mối liên quan với các feature khác.

**VD**: Dataset [Automobile](https://www.kaggle.com/datasets/toramky/automobile-dataset) chứa giá cả `price` và đặc điểm của nhiều loại xe khác nhau. Mục tiêu là dự đoán giá cả khi biết các đặc điểm của xe.

Sklearn hỗ trợ 2 loại MI là `mutual_info_regression` dùng cho numerical target và `mutual_info_classif` dùng cho categorical target. Trong trường hợp của Automobile, `price` là numerical nên ta sẽ dùng `mutual_info_regression`.

```python
from sklearn.feature_selection import mutual_info_regression

X = df.copy()
y = X.pop("price")
discrete_features = X.dtypes == int

def make_mi_scores(X, y, discrete_features):
    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
    mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores

mi_scores = make_mi_scores(X, y, discrete_features)
mi_scores[::3]  # show a few features with their MI scores
```
```sh
curb_weight          1.540126
highway_mpg          0.951700
length               0.621566
fuel_system          0.485085
stroke               0.389321
num_of_cylinders     0.330988
compression_ratio    0.133927
fuel_type            0.048139
Name: MI Scores, dtype: float64
```

Trực quan hóa cho dễ nhìn:

```python
def plot_mi_scores(scores):
    scores = scores.sort_values(ascending=True)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title("Mutual Information Scores")

plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(mi_scores)
```

![[mi-score-automobile-.png]]

`curb_weight` là feature có quan hệ chặt chẽ với `price` nhất.

# Creating Features

Dữ liệu của feature cũng ảnh hưởng đến chất lượng model. Dữ liệu phải được biến đổi sau cho phù hợp với yêu cầu của model và nghiệp vụ.

## Mathematical transform

Các phép biến đổi dữ liệu nhằm làm **giảm ảnh hưởng của độ lớn, làm nổi bật quan hệ giữa các feature**, hữu dụng đối với các model quan tâm đến quan hệ giữa các data vector (như *KNN, SVG,...*).
- **Reshaping**: Dùng hàm logarith, nén thang đó, biến quan hệ nhân thành quan hệ cộng.
- **Rescaling**: Thay đổi phạm vi giá trị của feature, thường là quy về \[0; 1] hay \[-1; 1].
- **Standardization / Normalizing**: Đưa dữ liệu về phân phối chuẩn.

Xem thêm các transformer có trong sklearn tại [[5. Data cleaning]].

**VD**: Reshaping:
```python
# If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log
accidents["LogWindSpeed"] = accidents.WindSpeed.apply(np.log1p)

# Plot a comparison
fig, axs = plt.subplots(1, 2, figsize=(8, 4))
sns.kdeplot(accidents.WindSpeed, shade=True, ax=axs[0])
sns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1]);
```

![[log-transform-wind.png]]

**VD** Min-max scaling: Quy dữ liệu về \[0; 1]:
```python
from mlxtend.preprocessing import minmax_scaling

# mix-max scale the data between 0 and 1
scaled_data = minmax_scaling(original_data, columns=[0])

# plot both together to compare
fig, ax = plt.subplots(1, 2, figsize=(15, 3))
sns.histplot(original_data, ax=ax[0], kde=True, legend=False)
ax[0].set_title("Original Data")
sns.histplot(scaled_data, ax=ax[1], kde=True, legend=False)
ax[1].set_title("Scaled data")
plt.show()
```

![[min-max-scaling.png]]

**VD**: Normalization:
```python
from scipy import stats

# normalize the exponential data with boxcox
normalized_data = stats.boxcox(original_data)

# plot both together to compare
fig, ax=plt.subplots(1, 2, figsize=(15, 3))
sns.histplot(original_data, ax=ax[0], kde=True, legend=False)
ax[0].set_title("Original Data")
sns.histplot(normalized_data[0], ax=ax[1], kde=True, legend=False)
ax[1].set_title("Normalized data")
plt.show()
```

![[normalization.png]]

## Categorical transform

#### Ordinal encoding
- Là ánh xạ mỗi giá trị trong feature thành một giá trị số tăng dần.
- Nếu không kiểm soát mapping đúng cách, model dễ đưa ra các dự đoán sai vì nhầm tưởng các category đang theo 1 thứ tự nào đó, dù thực tế có thể không (*downstream model*).

VD: Pandas: Dùng `.map` hay `.apply` đều tốt.
```python
df = pd.DataFrame({
    "color": ["red", "blue", "red", "green"]
})

color_map = {
    "red": 1,
    "blue": 2,
    "green": 3
}

df["color_id"] = df["color"].apply(lambda x: color_map.get(x, -1))
```

**VD**: Sklearn:
```python
from sklearn.preprocessing import OrdinalEncoder

ordinal_encoder = OrdinalEncoder()
encoded_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])
encoded_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])
```

#### One-hot encoding
- Là biến đổi mỗi giá trị trong feature thành một one-hot vector. One-hot vector là vector chỉ có 1 bit `1`, còn lại là `0`.
- Trong nhiều trường hợp, các one-hot vector sẽ tạo thành *ma trận thưa (sparse matrix)*, gây lãng phí bộ nhớ.

**VD**: Pandas:
```python
dummies = pd.get_dummies(df, columns=["color"])
```
```sh
   color_blue  color_green  color_red
0           0            0          1
1           1            0          0
2           0            0          1
3           0            1          0
```

**VD**: Sklearn:
```python
# Get list of categorical variables
categorical_cols = [col for col in X_train.columns if X_train[col].dtype == "object"]

from sklearn.preprocessing import OneHotEncoder

# One-hot encode categorical columns
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

encoded_X_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))
encoded_X_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))

# Remove categorical columns
num_X_train = X_train.drop(categorical_cols, axis=1) # Dù là Low-cardinality thì vẫn drop hết
num_X_valid = X_valid.drop(categorical_cols, axis=1)

# Concatenate numerical + encoded categorical features
OH_X_train = pd.concat([num_X_train, encoded_X_train], axis=1)
OH_X_valid = pd.concat([num_X_valid, encoded_X_valid], axis=1)

# Ensure column names are strings
OH_X_train.columns = OH_X_train.columns.astype(str)
OH_X_valid.columns = OH_X_valid.columns.astype(str)

```

| Ordinal encoder          | One-hot encoder          |
| ------------------------ | ------------------------ |
| ![](ordinal-encoder.png) | ![](one-hot-encoder.png) |

#### Imputer

Áp dụng cho các agreeate:

**VD**:
```python
# Mean encoding, Bin counting
autos["make_encoded"] = autos.groupby("make")["price"].transform("mean")

# 1. Gôm nhóm theo 'make'
# 2. Với mỗi nhóm, lấy 'mean' của 'price' để mã hóa cho từng giá trị của 'make'
```

Kết quả:

| --  | make        | price | make_encoded |
| --- | ----------- | ----- | ------------ |
| 0   | alfa-romero | 13495 | 15498.333333 |
| 1   | alfa-romero | 16500 | 15498.333333 |
| 2   | alfa-romero | 16500 | 15498.333333 |
| 3   | audi        | 13950 | 17859.166667 |
| 4   | audi        | 17450 | 17859.166667 |
| 5   | audi        | 15250 | 17859.166667 |
| 6   | audi        | 17710 | 17859.166667 |
| 7   | audi        | 18920 | 17859.166667 |
| 8   | audi        | 23875 | 17859.166667 |
| 9   | bmw         | 16430 | 26118.750000 |

#### Safe encoding

Nếu model gặp những vector có label chưa từng xuất hiện trong category thì sẽ bị lỗi.
Giải pháp là chỉ encode các category "an toàn", phổ biến.

```python
# Categorical columns in the training data
categorical_cols = [col for col in X_train.columns if X_train[col].dtype == "object"]

# Columns that can be safely ordinal encoded
# Are the columns that exists on both train and test set
good_label_cols = [col for col in object_cols if 
                   set(X_valid[col]).issubset(set(X_train[col]))]

# Problematic columns that will be dropped from the dataset
bad_label_cols = list(set(object_cols)-set(good_label_cols))
```

#### Low cardinality categorical encoding

Một nhược điểm dĩ nhiên của one-hot encoding là **tốn dung lượng**.
Giải pháp được đưa ra là:
- Tìm các feature mới thay thế categorical feature (*giảm cấp chiều dữ liệu, sẽ được giới thiệu sau*).
- Chỉ one-hot encoding với các feature có ít category. Các feature khác có thể bỏ đi hoặc tìm cách encode khác.

```python
# Columns that will be one-hot encoded
low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]

# Columns that will be dropped from the dataset
high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))
```


**Nhược điểm của mean encoding**:
- Nếu model gặp những vector có label chưa từng xuất hiện trong category thì sẽ bị lỗi. Giải pháp là đảm bảo model được training với tất cả label ngay từ đầu.
- Nhạy cảm với các dữ liệu nhiễu (*noises*). Giải pháp là **Smoothing**:
	- Công thức: $\text{encoded}(c)=w\times\text{mean}(T\;|\;C=c)+(1-w)\times\text{mean}(T)$.
	- Trong đó:
		- $c\in C$: Giá trị cần mã hóa.
		- $C$: Feature categorical.
		- $T$: Feature mục tiêu (*để tính mean*).
		- $\text{mean}(T\;|\;C=v)$: Trung bình giá trị của $T$ khi category là $c$.
		- $\text{mean}(T)$: Trung bình giá trị của $T$ (trên toàn bộ dataset).
		- $w\in[0;1]$: Hệ số tin cậy.
	- $w$ càng cao thì càng tin vào các "noise".
	- Thường thì $w=\dfrac{n}{n+m}$ (*m-estimate*), với $n$ là số lần $v$ xuất hiện và $m\geq0$ là *hệ số smooth*. $m$ gần như tỷ lệ nghịch với $w$.

Ở khía cạnh trực quan hóa, phân phối của $C$ sẽ càng sát với phân phối của $T$ nếu $m$ càng nhỏ. Trên thực tế, người ta thường xây dựng tập ứng cử viên $m\in\{0;\;5;\;10;\;50;\;100;\;...\}$ rồi đánh giá bằng cross-validation.

![](https://storage.googleapis.com/kaggle-media/learn/images/1uVtQEz.png)

VD: Dataset [Movielens 20M](https://www.kaggle.com/datasets/grouplens/movielens-20m-dataset) chứa hơn 20 triệu records về đánh giá của người dùng về các phim trên nền tảng Movielens. Trong đó, feature `Zipcode` chứa hơn 3000 category. Ta sẽ thử mã hóa feature này với cơ chế *Smoothing* với target là `Rating`.

```python
X = df.copy()
y = X.pop('Rating')

# We will use 25% data for categorical encoding
# 75% for training 

X_encode = X.sample(frac=0.25) # Other features
y_encode = y[X_encode.index]   # 'Rating'

X_pretrain = X.drop(X_encode.index)
y_train = y[X_pretrain.index]

# --
from category_encoders import MEstimateEncoder

# Create the encoder instance. Choose m to control noise.
encoder = MEstimateEncoder(cols=["Zipcode"], m=5.0)

encoder.fit(X_encode, y_encode)

# Encode the Zipcode column to create the final training data
X_train = encoder.transform(X_pretrain)

# Plot
plt.figure(dpi=90)
ax = sns.distplot(y, kde=False, norm_hist=True)
ax = sns.kdeplot(X_train.Zipcode, color='r', ax=ax)
ax.set_xlabel("Rating")
ax.legend(labels=['Zipcode', 'Rating']);
```

![[smoothing-movielens.png]]

## Building-up & Breaking-down

Nhiều feature có thể **gộp lại (building-up)** thành 1 feature lớn hơn.

Một feature có thể được **chia nhỏ (breaking-down)** thành nhiều feature đơn giản hơn.

**VD**: Các feature sau:
- ID numbers: `'123-45-6789'`.
- Phone numbers: `'(999) 555-0123'`.
- Street addresses: `'8241 Kaggle Ln., Goose City, NV'`.
- Internet addresses: `'http://www.kaggle.com.`
- Product codes: `'0 36000 29145 2'`.
- Dates and times: `'Mon Sep 30 07:06:05 2013'`.

- Phone numbers có phần tiền tố chỉ định khu vực (`999`) luôn giống nhau ở mọi record, có thể xóa đi để giảm tải dữ liệu.
- Street addresses có thể chia ra thành: Home address (`8241`), Street name (`Kaggle Ln.`), City name (`Goose City`), Country name (`NV`).

**VD**:
```python
df = pd.DataFrame({
    "day-month-year": ["03-07-2020", "15-12-2021", "01-01-2022"]
})

df[["day", "month", "year"]] = df["day-month-year"].str.split("-", expand=True)
df.drop(columns=['day-month-year'])
```
```sh
  day month  year
0  03    07  2020
1  15    12  2021
2  01    01  2022
```

## Principal Component Analysis (PCA)

**PCA** là thuật toán giảm độ phức tạp của dữ liệu (*Dimensionality reduction*).

**Ý tưởng của PCA**:
- Tìm các **Axes of variation**: Các trục thể hiện sự biến thiên của dữ liệu.
	- Các trục này phải vuông gốc với nhau.
	- Các trục này phải là tổ hợp tuyến tính (*linear combination*) của các feature gốc với hệ số tải (*loading*). Dấu của hệ số tải thể hiện quan hệ tỷ lệ giữa axes of variation với các features gốc.
- Có thể sử dụng các trục này làm feature mới. Các feature này gọi là **Principal component (PC)**.

**Ý nghĩa**:
- **PC có thể nói lên được nhiều điều về dữ liệu**, vì chúng là tổ hợp của các feature gốc.
- **Một dataset có thể có rất nhiều PC**, ta chỉ chọn lọc các PC thể hiện nhiều đặc điểm của dữ liệu nhất để làm feature.
- **PC là các feature đáng được cân nhắc để đưa vào training set**. Chú ý rằng PC càng có ý nghĩa thì không nhất thiết đem lại chất lượng cao cho model, nó chỉ thể hiện rằng feature này nói lên được nhiều điều về model. Danh sách các feature đưa vào training set còn cần xem xét đến các yếu tố như MI score, model's features,...
- **Giảm chiều dữ liệu (Dimensionality reduction)**: Một PC có thể thay thế nhiều feature, do đó giảm độ phức tạp của dữ liệu.

**Chú ý**: Dữ liệu cần standardize trước khi kiểm tra PCA.

**VD**: Xem xét dataset [Abalone](https://www.kaggle.com/datasets/rodolfomendes/abalone-dataset) sau, bạn có thể thấy dữ liệu dường như phân bố theo một đường thẳng nào đó (*axes of variation*).

![](https://storage.googleapis.com/kaggle-media/learn/images/rr8NCDy.png)

Trong dataset này, có 2 axes of variation:
- **Size**: Tỷ lệ thuận với Diameter và Height.
- **Shape**: Tỷ lệ nghịch với Diameter nhưng tỷ lệ thuận với Height.

Trên thực tế, Size và Shape là các tổ hợp tuyến tính của Diameter và Height theo công thức sau:
```
Size  = 0.707 * Height + 0.707 * Diameter
Shape = 0.707 * Height - 0.707 * Diameter
```

Mức độ ngữ nghĩa giữa 2 PC:
![](https://storage.googleapis.com/kaggle-media/learn/images/xWTvqDA.png)

**VD**: Dataset [Automobile](https://www.kaggle.com/datasets/toramky/automobile-dataset) chứa đặc điểm của nhiều loại xe khác nhau. Ta sẽ kiểm tra các PC của dataset này.

1 - Từ các feature có MI cao ở phần 1, ta standardize trước:
```python
features = ["highway_mpg", "engine_size", "horsepower", "curb_weight"]

X = df.copy()
y = X.pop('price')
X = X.loc[:, features]

# Standardize
X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)
```

2 - Sử dụng phương thức `sklearn.decomposition.PCA`:
```python
from sklearn.decomposition import PCA

# Create principal components
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Convert to dataframe
component_names = [f"PC{i+1}" for i in range(X_pca.shape[1])]
X_pca = pd.DataFrame(X_pca, columns=component_names)

loadings = pd.DataFrame(
    pca.components_.T,  # transpose the matrix of loadings
    columns=component_names,  # so the columns are the principal components
    index=X.columns,  # and the rows are the original features
)
loadings
```

|             | PC1       | PC2      | PC3       | PC4       |
| ----------- | --------- | -------- | --------- | --------- |
| highway_mpg | -0.492347 | 0.770892 | 0.070142  | -0.397996 |
| engine_size | 0.503859  | 0.626709 | 0.019960  | 0.594107  |
| horsepower  | 0.500448  | 0.013788 | 0.731093  | -0.463534 |
| curb_weight | 0.503262  | 0.113008 | -0.678369 | -0.523232 |

Kết quả trả về là một ma trận hệ số tải. Thử minh họa trực quan bằng chart:

```python
# Look at explained variance
plot_variance(pca)
```

![[plot-variance-automobile.png]]

## Bag of Words (BoW)

BoW là một kỹ thuật xử lý dữ liệu đầu vào ở dạng văn bản. Với một chuỗi văn bản đầu vào:
1. Xây dựng một **từ điển (Dictionary, Codebook)** là danh sách các từ được dùng. Từ điển này có $D$ từ.
2. Chuỗi văn bản đầu vào có thể được vector hóa thành vector $x = [x_1, x_2, ..., x_D]$, với $x_i$ là số lần xuất hiện từ thứ $i$ trong chuỗi đầu vào.
3. 2 từ được cho là có nghĩa giống nhau khi **khoảng cách Euclid** của nó khá nhỏ.

Lưu ý: Trên thực tế.
- Vector này sẽ rất dài. Các từ không xuất hiện trong chuỗi đầu vào thì sẽ tạo ra phần tử bằng 0. Nếu có nhiều từ như thế, vector tạo được sẽ **thưa (sparce vector)**. Để việc lưu trữ được hiệu quả hơn, ta không lưu cả vector đó mà *chỉ lưu vị trí của các phần tử khác 0 và từ tương ứng*.

- Nếu có từ đầu vào không có trong từ điển, thì ta có thể thêm một phần tử `<Unknown>` vào cuối từ điển. Những từ không có trong từ điển thì được xem là Unknown.

- **Term frequency-inverse document frequency (TFIDF)** là kỹ thuật xác định độ quan trọng của từ. Có những từ tuy hiếm nhưng lại thể hiện ý nghĩa chính trong văn bản.

- BoW không thể hiện được thứ tự và ngữ nghĩa của các từ. Có những chuỗi có vector giống nhau nhưng ý nghĩa thì khác nhau.

BoW còn có thể được mở rộng sang mã hóa các loại dữ liệu khác.

VD: Có một bài toán cần phân loại đâu là ảnh rừng, đâu là ảnh sa mạc. Giả sử dữ liệu vào chỉ có thể là ảnh rừng hoặc sa mạc.
- Bằng một cách trực quan, ta thấy những tấm ảnh rừng thì thường có màu xanh lá, còn sa mạc thì có màu đỏ hoặc vàng. Xanh, đỏ, vàng có thể coi là từ điển.
- Với mỗi bức ảnh, ta xây dựng một vector dựa trên từ điển trên. Với mỗi điểm ảnh, ta đếm xem nó gần với loại màu sắc nào. Vector này còn được gọi là **histogram vector**.

Tuy nhiên, nếu gặp phải những bức ảnh không màu thì phương pháp trên vô dụng. Một giải pháp được đưa ra là dùng patch.
- **Patch** là một vùng chữ nhật trên ảnh.
- Từ điển sẽ chứa các path, mỗi path thể hiện các đặc trưng như cồn cát, thân cây, lá cây,...
