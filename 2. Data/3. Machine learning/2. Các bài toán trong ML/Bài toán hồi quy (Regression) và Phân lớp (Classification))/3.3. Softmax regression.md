
# Cơ sở Toán học của Softmax regression

Trong bài toán phân lớp nhị phân thì hàm sigmoid có thể cho ra output là xác suất để điểm $\mathbf{x}_k$ thuộc lớp $i$, thỏa mãn các xác suất này không âm và tổng của chúng là 1.

Tuy nhiên, trong bài toán áp dụng phân lớp nhị phân sang phân lớp đa lớp thì xác suất trên không còn đúng nữa, chúng chỉ đúng với phân lớp nhị phân thôi. Hàm Softmax đã ra đời giúp ta chuẩn hóa lại các xác suất trong bài toán này.

Mô hình Softmax như sau:

$$
\begin{align}
p(y_k=i|\mathbf{x}_k;\mathbf{W})=a_i&=\frac{\exp(z_i)}{\sum_{j=1}^C\exp{(z_j)}}\\
&=\boxed{\frac{\exp(z_i-c)}{\sum_{j=1}^C\exp{(z_j-c)}}}
\end{align}
$$

Trong đó:
- $\mathbf{W}=[\mathbf{w}_1,...\mathbf{w}_C]$ là ma trận trọng số. Vì đầu ra của ta lúc này có nhiều output.
- $p(y_k=i|\mathbf{x}_k;\mathbf{W})$ là xác suất điểm $\mathbf{x}_k$ rơi vào class $i$.
- $a_i$ là hàm số Softmax.
- $c\geq0$ là hằng số hiệu chỉnh giá trị hàm Softmax. Khi $c=0$, hàm Softmax có thể có tính tới giá trị rất lớn do nó chứa $\exp$, dẫn đến tràn số. Thường thì $c=\max{z_i}$.

Mô hình Softmax dưới dạng Neural network:
![](https://machinelearningcoban.com/assets/13_softmax/softmax_nn.png)

Ví dụ cho thấy mối quan hệ giữa $z_i$ và $a_i$:
![](https://machinelearningcoban.com/assets/13_softmax/softmax_ex.png)
Ta thấy trong các trường hợp, hàm Softmax đã chuẩn hóa lại các xác suất cho hợp lý.

**Hàm mất mát** của Softmax regression được xây dựng từ độ khác nhau giữa phân phối xác suất $a_i$ và $z_i$, còn gọi là **Cross-entropy**:
$$
\begin{align}
H(\mathbf{p},\mathbf{q})&=\mathbf{E}_\mathbf{p}[-\log{\mathbf{q}}]\\
&=\boxed{-\sum_{i=1}^Cp_i\log{q_i}}
\end{align}
$$
Vậy hàm mất mát của Softmax sẽ là:
$$
\boxed{J(\mathbf{W};\mathbf{X},\mathbf{Y})=-\sum_{i=1}^N\sum_{j=1}^Cy_{ij}\log{a_{ij}}}
$$

Ta vẫn tối ưu $\mathbf{W}$ bằng GD:
$$\boxed{\mathbf{W}\leftarrow\mathbf{W}+\eta.\mathbf{x}_i.(\mathbf{y}_i-\mathbf{a}_i)^T}$$

# Thí nghiệm
















