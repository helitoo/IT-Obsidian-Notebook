
```insta-toc
---
title:
  name: Mục lục
  level: 1
  center: false
exclude: ""
style:
  listType: number
omit: []
levels:
  min: 1
  max: 6
---

# Mục lục

1. Batch GD
    1. GD cơ bản
    2. Momentum
    3. Nesterov accelerated gradient (NAG)
2. Stochastic GD (SGD)
```


Trong ML nói riêng và bài toán tối ưu nói chung, ta phải thường xuyên tìm các giá trị $\min$, $\max$. Tuy nhiên, việc này thường rất phức tạp hay có thể có vô số nghiệm.

Hướng tiếp cận khả quan hơn là xuất phát từ 1 điểm $\theta_t$ được coi là gần với nghiệm $\theta^*$, sau đó dùng phép lặp để tiến sát đến $\theta^*$, tức là tiến gần với đạo hàm bằng 0.

Thuật toán dừng lặp khi:
1. Lặp với số lần cố định.
2. Ước lượng một nghiệm đủ nhỏ cho trước, nếu hội thụ tới điểm này thì dừng lại.
3. Ước lượng một độ mất mát đủ nhỏ cho trước, nếu hội tụ tới điểm đạt độ mất mát đó thì dừng lại.

# Batch GD

**Batch GD** là nhóm các thuật toán hội tụ theo phương pháp: Với mỗi điểm $\theta_t$, tính độ mất mát toàn bộ tập dữ liệu với bộ tham số $\theta_t$ để kiểm tra nghiệm.

## GD cơ bản

Công thức:
$$\theta_{t+1}=\theta_t-\eta.\nabla_\theta.J(\theta_t)$$

Cách viết khác:
$$\boxed{\theta\leftarrow \theta-\eta.\nabla_\theta.J(\theta)}$$

Trong đó, $\eta$ là **tốc độ học** (learning rate). Dấu trừ thể hiện rằng $\theta_{t+1}$ phải *đi ngược lại hướng đạo hàm*. Giả sử như $\nabla_\theta.J(\theta_t)>0$, tức là $\theta_t$ đang ở bên phải $\theta^*$, nên $\theta_{t+1}$ phải di chuyển về bên trái.

$\eta$ càng nhỏ thì tốc độ hội tụ nghiệm càng chậm. $\eta\geq0.5$ thì được coi là cao.

## Momentum

Thuật toán GD truyền thống có nhược điểm là:
- Tốc độ hội tụ khá ổn định và nhìn chung là chậm.
- Nếu có nhiều điểm cực tiểu, $\theta$ dễ bị mắc kẹt tại 1 điểm cực tiểu mà không tiếp cận được với nghiệm thực sự.

Momentum là một dạng tối ưu hơn. Trong momentum, $\theta$ trông "vật lý" hơn với 2 đặc điểm:
- Tốc độ hội tụ nhanh nếu đạo hàm lớn (độ dốc cao).
- $\theta$ có khả năng vượt qua các điểm cực tiểu mà không phải nghiệm.

![](https://machinelearningcoban.com/assets/GD/momentum.png)

Công thức:
$$
\begin{align}
\theta&\leftarrow\theta-v_t\\
&=\boxed{\theta-\eta.\nabla_\theta.J(\theta)-\gamma.v_{t-1}}
\end{align}
$$

Trong đó, $v_t$ là tốc độ hội tụ của $\theta$, thường thì $v_0=0$.

## Nesterov accelerated gradient (NAG)

Momentum vẫn có nhược điểm, nó tỏ ra hội tụ chậm dù đã đến nghiệm thật sự rồi, giống như một viên bi cứ lăn qua lăn lại ở đáy nhiều lần trước khi dừng hẳn.

NAG là một phương pháp khắc phục nhược điểm trên thông qua việc **dự đoán nghiệm**.

Công thức:
$$
\begin{align}
\theta&\leftarrow\theta-v_t\\
&=\boxed{\gamma.v_{t-1}+\eta.\nabla_\theta.J(\theta-\gamma.v_{t-1})}
\end{align}
$$

# Stochastic GD (SGD)

Do các thuật toán xử lý theo batch tính toán độ mất mát trên toàn bộ tập dữ liệu ở mỗi lần cập nhật $\theta$ nên độ chính xác cao, nhưng bù lại thì tốc độ khá chậm, không phù hợp với tập dữ liệu quá lớn hay cập nhật liên tục.

SGD chỉ tính toán độ mất mát tại một điểm dữ liệu, hy sinh một chút độ mất mát, nhưng nhìn chung thì nghiệm vẫn hội tụ đúng.

Công thức:
$$\boxed{\theta\leftarrow\theta-\eta.\nabla_\theta.J(\theta,\mathbf{x}_i)}$$

Trong đó $\mathbf{x}_i$ là một điểm ngẫu nhiên (*Stochastic*) trong tập dữ liệu.

Ngoài kiểu vận dụng như trên, SGD còn có một dạng là **mini-batch**, tức là tính toán độ mất mát trên một số lượng điểm dữ liệu $>1$ nhưng $<N$.























