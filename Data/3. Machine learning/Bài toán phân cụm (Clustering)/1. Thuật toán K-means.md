
```insta-toc
---
title:
  name: Mục lục
  level: 1
  center: false
exclude: ""
style:
  listType: number
omit: []
levels:
  min: 1
  max: 6
---

# Mục lục

1. Cơ sở Toán học của bài toán phân cụm
2. Thuật toán K-means
3. Hạn chế của K-means clustering
```

![](https://contenthub-static.grammarly.com/blog/wp-content/uploads/2025/02/Clustering_Data.png)

# Cơ sở Toán học của bài toán phân cụm

**Cụm (Cluster)** là tập hợp các vector đặc trưng *gần nhau*, thường dựa vào khoảng cách Euclid. **Bài toán phân cụm** là phân $N$ vector thành $K<N$ cụm riêng biệt.

Gọi:
- Tập hợp các vector đầu vào: $\mathbf{X}\in\mathbb{R}^{d\times N}$.
- Tập hợp các tâm của cụm (**Centroid**): $\mathbf{M}\in\mathbb{R}^{d\times K}$.
- Mỗi cụm được đánh dấu bằng một **nhãn (label)** $k_i\in[1,2,3,...,K]$. Trên thực tế, các nhãn được biểu diễn dưới dạng **one-hot**, tức là $\mathbf{Y}\in\mathbb{R}^{N\times K}$, trong đó:
	- Vector $\mathbf{y}_i\in\mathbf{Y}$ là một one-hot vector.
	- Nếu $y_{ik}=1$ thì vector $\mathbf{x}_i$ thuộc cụm có label $k$ hoặc thuộc cụm có tâm $\mathbf{m}_k$, ngược lại thì $y_{i,j\neq k}=0$.

**Hàm mất mát**:
$$
\begin{align}
\mathcal{L}(\mathbf{Y},\mathbf{M})&=\frac{1}{N}\sum_{i=1}^N\sum_{k=1}^K\|\mathbf{x}_i-\mathbf{m}_k\|_2^2\\
&=\boxed{\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^Ky_{ij}\|\mathbf{x}_i-\mathbf{m}_j\|_2^2}
\end{align}
$$

Hàm trên là một dạng mix-integer programming và rất khó để tìm nghiệm tối ưu. Có một phương pháp tìm nghiệm tối ưu đó là lần lượt cố định $\mathbf{Y}$ và $\mathbf{M}$, sau đó giải tìm biến còn lại.

**Tìm nghiệm tối ưu bằng cách tìm $\mathbf{Y}$ và cố định $\mathbf{M}$**: Đồng nghĩa với việc ta đã biết trước các tâm cụm, và việc của ta chỉ là tìm các nhãn $\mathbf{y}_i$ cho phù hợp cho từng điểm dữ liệu.
$$\mathbf{y}_i=\boxed{\arg\min_{\mathbf{y}_i}\sum_{j=1}^K\|\mathbf{x}_i-\mathbf{m}_j\|_2^2}$$
Nói cách khác, vector $\mathbf{y}_i$ được phân vào cụm có tâm cụm $\mathbf{m}_j$ gần nó nhất.

**Tìm nghiệm tối ưu bằng cách tìm $\mathbf{M}$ và cố định $\mathbf{Y}$**: Đồng nghĩa với việc ta đã không biết trước các tâm cụm, và việc của ta là tìm các tâm cụm đó.
$$
\begin{align}
\mathbf{m}_j&=\arg\min_{\mathbf{m}_j}
\sum_{i=1}^N\|\mathbf{x}_i-\mathbf{m}_j\|_2^2\\
&=\boxed{\arg\min_{\mathbf{m}_j}\frac{\sum_{i=1}^Ny_{ij}\mathbf{x}_i}{\sum_{i=1}^Ny_{ij}}}
\end{align}
$$
Trong đó:
- $\sum_{i=1}^Ny_{ij}$ là số lượng các vector trong cụm $j$.
- $\sum_{i=1}^Ny_{ij}\mathbf{x}_i$ là tổng các vector trong cụm $j$.
Nói cách khác, vector $\mathbf{m}_j$ là tâm cụm của các vector $\mathbf{x}_i$ sao cho trung bình cộng khoảng cách của nó với các vector $\mathbf{x}_i$ là nhỏ nhất.

# Thuật toán K-means

**K-means** là thuật toán phân cụm với số lượng cụm biết trước là $K$. Các bước của thuật toán:
1. Chọn $K$ điểm ngẫu nhiên làm tâm cụm.
2. Phân mỗi vector $\mathbf{x}_i$ vào tâm cụm gần nó nhất.
3. Tính lại tâm cụm mới.
4. Nếu tâm cụm mới giống với tâm cụm cũ thì thuật toán kết thúc.

**Thí nghiệm**: Cho 500 điểm ngẫu nhiên. Hãy phân chúng thành 3 cụm $\mathbf{X}=[\mathbf{x}_0;\mathbf{x}_1;\mathbf{x}_2]$ với mỗi cụm có kỳ vọng và ma trận hiệp phương sai như sau:

```python
import numpy as np
from scipy.spatial.distance import cdist
import random np.random.seed(18)

means = [[2, 2], [8, 3], [3, 6]]
cov = [[1, 0], [0, 1]]
N = 500
X0 = np.random.multivariate_normal(means[0], cov, N)
X1 = np.random.multivariate_normal(means[1], cov, N)
X2 = np.random.multivariate_normal(means[2], cov, N)
X = np.concatenate((X0, X1, X2), axis = 0)

K = 3 
```

Các hàm bổ trợ:
```python
# Trả về k tâm cụm ngẫu nhiên
def kmeans_init_centroids(X, k):
    return X[np.random.choice(X.shape[0], k, replace=False)]


# Trả về vector label Y
def kmeans_assign_labels(X, centroids):
	# Khoảng cách giữa mỗi vector X với mỗi centroid
    D = cdist(X, centroids)
    
    # Trả về index khoảng cách nhỏ nhất đối với mỗi vector
    return np.argmin(D, axis=1)


# So sánh độ giống nhau giữa centroid cũ và mới
def has_converged(centroids, new_centroids):
    return (set([tuple(a) for a in centroids]) ==
            set([tuple(a) for a in new_centroids]))


# Cập nhật centroid khi biết label
def kmeans_update_centroids(X, labels, K):
    centroids = np.zeros((K, X.shape[1]))
    
    for k in range(K):
        Xk = X[labels == k, :]                # Lấy tất cả điểm thuộc cụm k
        centroids[k, :] = np.mean(Xk, axis=0) # Tính centroid mới
    return centroids


# K-means
def kmeans(X, K):
	centroids = [kmeans_init_centroids(X, K)]
	labels = []
	
	while True:
		labels.append(kmeans_assign_labels(X, centroids[-1]))
		new_centroids = kmeans_update_centroids(X, labels[-1], K)
		
		if has_converged(centroids[-1], new_centroids):
			break
		centroids.append(new_centroids)
	
	return (centroids, labels)

(centroids, labels, it) = kmeans(X, K)
print('Centers found by our algorithm:\n', centroids[-1])
kmeans_display(X, labels[-1])
```
```sh
Centers found by our algorithm:
	[[ 1.9834967 1.96588127]
	[ 3.02702878 5.95686115]
	[ 8.07476866 3.01494931]]
```

Dùng Scikit-learn:
```python
from sklearn.cluster

import KMeans model = KMeans(n_clusters=3, random_state=0).fit(X)

print('Centers found by scikit-learn:')
print(model.cluster_centers_)

pred_label = model.predict(X)
kmeans_display(X, pred_label)
```
```sh
Centroids found by scikit-learn:
	[[ 8.0410628 3.02094748]
	[ 2.99357611 6.03605255]
	[ 1.97634981 2.01123694]]
```

![](https://machinelearningcoban.com/assets/kmeans/kmeans11.gif)

# Hạn chế của K-means clustering

1. Số lượng cụm cần được xác định trước. Có một phương pháp dự đoán $K$ là **elbow method**.
2. Chất lượng nghiệm phụ thuộc vào cụm khởi tạo ban đầu. Có thể hạn chế nhược điểm này bằng:
	1. Chạy K-means nhiều lần với các centroid ban đầu khác nhau, sau đó chọn nghiệm ít mất mát nhất.
	2. **K-Means++**: Phương pháp chọn centroid tối ưu.
3. Các cụm phải có số lượng điểm cân đối.
4. Các cụm phải có dạng gần cầu.
5. Các cụm phải tách biệt nhau.
