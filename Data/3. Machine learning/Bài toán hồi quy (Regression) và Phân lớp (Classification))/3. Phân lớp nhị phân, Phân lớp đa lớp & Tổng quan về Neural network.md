
```insta-toc
---
title:
  name: Mục lục
  level: 1
  center: false
exclude: ""
style:
  listType: number
omit: []
levels:
  min: 1
  max: 6
---

# Mục lục

1. Phân lớp nhị phân
2. Áp dụng phân lớp nhị phân vào phân lớp đa lớp
3. Tổng quan về Neural network
```

# Phân lớp nhị phân

Trong trường hợp phân lớp *chỉ có 2 lớp*, ngoài cách phân lớp bằng [[2.2. Naive Bayes Classifier (NBC)]], ta có thể phân lớp bằng cách hồi quy. Tức là tìm một phương trình **Boundary** ngăn cách giữa 2 lớp ấy.

![](https://machinelearningcoban.com/assets/pla/pla4.png)

- **PLA**: Boundary là phương trình tuyến tính thông thường.
- **LR**: Boundary là phương trình tuyến tính sigmoid.

Các phương pháp xây dựng hàm mất mát:
1. Dựa trên khoảng cách giữa các điểm misclassified đến boudary:
	$$J(\mathbf{w})=\sum_{\mathbf{x}_i\in\mathcal{M}}(-y_i\mathbf{w}^T\mathbf{x}_i)$$
2. Dựa trên xác suất điểm $\mathbf{x}_i$ rơi vào đúng class của nó:
	$$J(\mathbf{w})=P(\mathbf{y}|\mathbf{X};\mathbf{w})=\prod_{i=1}^NP(y_i|\mathbf{x}_i;\mathbf{w})$$

Phương pháp tìm $\mathbf{w}$ tối ưu đều dựa trên [[4. Thuật toán Gradient descent (GD)]].

# Áp dụng phân lớp nhị phân vào phân lớp đa lớp

Có **ít nhất bốn cách** để áp dụng các mô hình phân lớp nhị phân vào các bài toán phân lớp đa lớp.

1. **One-vs-one**: Xây dựng rất nhiều bộ phân lớp nhị phân cho từng cặp lớp dữ liệu. Cuối cùng, 1 điểm dữ liệu sẽ được phân vào lớp có xác suất rơi vào cao nhất.. Tuy nhiên, cần đến $\frac{C(C-1)}{2}$ lớp cần dùng, quá lớn, nên không khả thi.
2. **Phân tầng**: Chọn ra 2 lớp có tính "bao trùm" nhất, từ mỗi lớp này lại tiếp tục chọn thêm 2 lớp nữa,...
3. **Binary coding**: Là mã hóa các lớp $y_i$ thành các số nhị phân có 2 bit, sau đó dùng bộ phân lớp để chọn ra các điểm thuộc lớp bit thứ nhất, lớp bit thứ hai,... Nếu các lớp $y_i$ được mã hóa sang số nhị phân nhiều hơn 2 bit, thì ta lại áp dụng lồng các phương pháp này để phân lớp cho từng bit đó.
4. **One-vs-rest / One-hot coding** \[Thông dụng]: Nếu có $C$ lớp thì ta sẽ xây dựng $C$ bộ phân lớp nhị phân, bộ phân lớp thứ $i$ sẽ phân lớp $y_i$ và các lớp $y_{j\neq i}$ còn lại. Cuối cùng, điểm dữ liệu sẽ được phân vào lớp có xác suất rơi vào cao nhất.

# Tổng quan về Neural network

Nguyên lý hoạt động của Neural network cũng tương tự như bài toán phân lớp dựa trên boundary. Tức là với mỗi $\mathbf{x}_i\in\mathbf{X}$, ta gắn thêm cho nó $\mathbf{w}$ để xây dựng hàm boundary, trong Neutral network thì gọi là **Active function**.

>[!important]
>Deep learning là các mô hình Machine learning sử dụng Neural network.

**Sơ đồ Neutral network tương ứng với PLA**:

![](https://machinelearningcoban.com/assets/pla/pla_nn.png)

Trong đó:
- Các điểm $x_i\in\mathbf{x}\in\mathbb{R}^4$ là input, được ký hiệu bằng các node màu xanh lục, với $x_0=1$ là affine.
- Các input được gắn thêm $w_i$ tương ứng, tạo thành node $z=\sum_{i=1}^d(\mathbf{w}^Tx_i)$.
- Đầu ra là hàm $y=\text{sign}$. Ký hiệu bên trong node này là đồ thị của hàm $\text{sign}$.

**Sơ đồ Neutral network tương ứng với Linear regression**:

![](https://machinelearningcoban.com/assets/pla/lr_nn.png)


**Sơ đồ phân lớp đa lớp dựa trên phân lớp nhị phân**:

![](https://machinelearningcoban.com/assets/LogReg2/binaryclassifiers.png)

Lúc này, có hàng loạt node output được cho ra, các output này lại tiếp tục lồng vào nhau cho đến khi trở thành phân lớp nhị phân. Các output này được gọi là **Output layer**.
