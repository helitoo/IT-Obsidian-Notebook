
```insta-toc
---
title:
  name: Mục lục
  level: 1
  center: false
exclude: ""
style:
  listType: number
omit: []
levels:
  min: 1
  max: 6
---

# Mục lục

1. Giới thiệu KNN
2. Các vấn đề của KNN
    1. Khoảng cách từ một điểm tới từng điểm khác
    2. Khoảng cách giữa từng cặp điểm trong hai tập hợp
```

# Giới thiệu KNN

**K-nearest neighbors (Instance-based/Memory-based learning)** là một thuật toán supervised learning. Khi huấn luyện, thuật toán này không học một điều gì từ dữ liệu huấn luyện mà nhớ lại một cách máy móc toàn bộ dữ liệu đó.

Nói cách khác, nó **đi tìm đầu ra của một điểm dữ liệu mới bằng cách chỉ dựa trên thông tin (nhãn) của $K$ điểm dữ liệu gần nhất (Major voting)** trong tập huấn luyện. Có 2 loại major voting:
1. **Uniform weight**: Là nhãn xuất hiện nhiều nhất trong số các điểm hàng xóm.
2. **Distance weight**: Không chỉ đếm số lượng nhãn, mà còn tính đến xác suất hoặc khoảng cách với từng điểm hàng xóm. Thường thông qua trọng số, hàng xóm càng gần thì trọng số càng cao, tức là *nghịch đảo của khoảng cách tới hàng xóm*. Cách này thì dễ gây ra overfitting.

Đặc điểm:
- Dễ bị nhiễu, nhất là đối với distance voting.
- $K$ càng cao, độ chính xác càng cao, nhưng tốc độ tính toán giảm.

**VD**: Bài toán phân lớp: Dùng 3NN để phân lớp từng ô vuông dựa vào 3 điểm tròn gần nó nhất.
![](https://upload.wikimedia.org/wikipedia/commons/7/78/KNN_decision_surface_animation.gif)

**VD**: Bài toán hồi quy tuyến tính: Dùng 5NN để xác định đầu ra $\mathbf{y}$ với đầu vào $\mathbf{x}$ bằng cách xác định 5 điểm gần nó nhất.
![](https://scikit-learn.org/stable/_images/sphx_glr_plot_regression_001.png)

**Thí nghiệm**:

**Iris flower dataset** là một bộ dữ liệu nhỏ bao gồm thông tin của ba class hoa khác nhau: *Iris setosa*, *Iris virginica* và *Iris versicolor*. Mỗi class có 50 bông hoa với dữ liệu là bốn thông tin: *chiều dài, chiều rộng đài hoa (sepal), và chiều dài, chiều rộng cánh hoa (petal)*.

Hãy:
- Tách 150 điểm dữ liệu trong dataset ra thành hai tập huấn luyện (130 điểm) và kiểm thử (20 điểm).
- Dùng KNN để phân loại điểm dữ liệu với K và weight khác nhau. Dùng `accuracy_score` để so sánh độ chính xác của mô hình với nhãn gốc của dataset.

```python
import numpy as np

from sklearn import neighbors, datasets

from sklearn.model_selection
import train_test_split

from sklearn.metrics import accuracy_score

np.random.seed(7)

# LOAD IRIS DATASET

iris = datasets.load_iris()
iris_X = iris.data                   # Các điểm dữ liệu
iris_y = iris.target                 # 3 nhãn tương ứng với 3 loài hoa

print('Labels:', np.unique(iris_y))

# PHÂN CHIA THÀNH TRAINING SET VÀ TEST SET

X_train, X_test, y_train, y_test = train_test_split(
	iris_X,
	iris_y,
	test_size=130
)

print('Train size:', X_train.shape[0], ', test size:', X_test.shape[0])
```
```sh
Labels: [0 1 2]
Train size: 20 , test size: 130
```

1NN:
```python
model = neighbors.KNeighborsClassifier(n_neighbors = 1, p = 2)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("Accuracy of 1NN: %.2f %%" %(100*accuracy_score(y_test, y_pred)))
```
```
Accuracy of 1NN: 92.31 %
```

Trong đó:
- `neighbors.KNeighborsClassifier`: Phương thức phân loại điểm dựa trên KNN:
	- `n_neighbors`: $K$.
	- `p`: Bậc của norm.
	Mặc định phương thức này dùng weight là `"uniform"`.

- `.fit(X_train, y_train)`: Phương thức huấn luyện mô hình, thực chất là lưu trữ dữ liệu lại, khi cần phân loại sẽ lấy ra dùng (Memory-based).

- `.predict(X_test)`: Phân loại trên tập dữ liệu `X_test`.

7NN:
```python
model = neighbors.KNeighborsClassifier(n_neighbors = 7, p = 2) model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("Accuracy of 7NN: %.2f %%" %(100*accuracy_score(y_test, y_pred)))
```
```sh
Accuracy of 7NN: 93.85 %
```
Độ chính xác tăng lên khi $K$ tăng.

7NN với distance voting:
```python
model = neighbors.KNeighborsClassifier(
	n_neighbors = 7,
	p = 2,
	weights = ’distance’)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print("Accuracy of 7NN (1/distance weights): %.2f %%"
	%(100*accuracy_score(y_test, y_pred)))
```
```sh
Accuracy of 7NN (1/distance weights): 94.62 %
```

# Các vấn đề của KNN

KNN là một thuật toán lazy learning, **không có hàm mất mát và bài toán tối ưu** nào phải thực hiện trong quá trình huấn luyện. Mọi tính toán được thực hiện ở bước kiểm thử.

Vì KNN ra quyết định dựa trên các điểm gần nhất nên có hai vấn đề ta cần lưu tâm.
1. **Khoảng cách giữa các điểm được định nghĩa là gì**, thường dùng norm $l_2$.
2. **Khoảng cách trên phải được tính như thế nào mới hiệu quả**.

## Khoảng cách từ một điểm tới từng điểm khác

Khoảng cách Euclid từ một điểm $\mathbf{z}$ tới một điểm $\mathbf{x}_i\in\text{training set}$ được định nghĩa là $\|\mathbf{z}-\mathbf{x}_i\|_2$. Vì trong cách tính này có một bước phải tính căn bậc hai nên người ta thường tính $\|\mathbf{z}-\mathbf{x}_i\|_2^2$. Nếu việc tính khoảng cách chỉ để phục vụ việc sắp xếp thì ta không cần tính căn bậc hai sau bước này nữa.

Để ý rằng:
$$\|\mathbf{z}-\mathbf{x}_i\|_2^2=
(\mathbf{z}-\mathbf{x}_i)^T(\mathbf{z}-\mathbf{x}_i)
=\|\mathbf{x}_i\|^2_2+\|\mathbf{z}\|^2_2-2\mathbf{x}_i^T\mathbf{z}$$
Do đó:
- Nếu nhiệm vụ chỉ là tìm ra $\mathbf{x}_i$ gần với $\mathbf{z}$ nhất, số hạng đầu tiên có thể được bỏ qua.
- Nếu có nhiều điểm dữ liệu trong tập kiểm thử, các giá trị $\|\mathbf{x}_i\|^2_2$ có thể được tính và lưu trước vào bộ nhớ. Khi đó, ta chỉ cần tính các tích vô hướng $\mathbf{x}_i^T\mathbf{z}$.

VD:

Tính khoảng cách dựa trên $\|\mathbf{z}-\mathbf{x}_i\|_2^2$:
```python
def dist_pp(z, x):
	d = z - x.reshape(z.shape)
	return np.sum(d*d)


def dist_ps_naive(z, X):
	N = X.shape[0]
	res = np.zeros((1, N))
	for i in range(N):
		res[0][i] = dist_pp(z, X[i])
	
	return res
```

Tính khoảng cách dựa trên $\|\mathbf{x}_i\|^2_2+\|\mathbf{z}\|^2_2-2\mathbf{x}_i^T\mathbf{z}$:
```python
def dist_ps_fast(z, X):
	X2 = np.sum(X*X, 1)
	z2 = np.sum(z*z)
	return X2 + z2 - 2*X.dot(z)
```

Với tập dữ liệu sau:
```python
import numpy as np
d, N = 1000, 10000        # số chiều và số điểm dữ liệu X
X = np.random.randn(N, d) # Tạm cho random giá trị của các điểm
```
- `dist_ps_naive` tốn khoảng `0.09` s.
- `dist_ps_fast` tốn khoảng `0.05` s, nhanh hơn nhiều so với cách tính truyền thống.

## Khoảng cách giữa từng cặp điểm trong hai tập hợp

Thông thường, ta không những phải tính khoảng cách từ một điểm $\mathbf{z}$ tới tập hợp các điểm $X$, mà thường xuyên còn phải tính khoảng cách giữa nhiều điểm $Z$ tới $X$.

VD:

Cách tính truyền thống, tính từng điểm:
```python
def dist_ss_0(Z, X):
	M = Z.shape[0]
	N = X.shape[0]
	
	res = np.zeros((M, N))
	for i in range(M):
		res[i] = dist_ps_fast(Z[i], X)
	
	return res
```

Tính dựa trên $\|X\|^2_2+\|Z\|^2_2-2X^TZ$:
```python
def dist_ss_fast(Z, X):
	X2 = np.sum(X*X, 1)
	Z2 = np.sum(Z*Z, 1)
	return Z2.reshape(-1, 1) + X2.reshape(1, -1) - 2*Z.dot(X.T)
```
