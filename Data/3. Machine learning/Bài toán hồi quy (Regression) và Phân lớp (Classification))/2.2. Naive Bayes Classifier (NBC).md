
```insta-toc
---
title:
  name: Mục lục
  level: 1
  center: false
exclude: ""
style:
  listType: number
omit: []
levels:
  min: 1
  max: 6
---

# Mục lục

1. Cơ sở Toán học của Bài toán phân lớp và NBC
2. Thí nghiệm
```


# Cơ sở Toán học của Bài toán phân lớp và NBC

Gọi:
- $C$ là số lượng các label (số lượng lớp).
- Mỗi lớp $c\in\{1,2,...,C\}$.

Đối với bài toán phân lớp, thay vì tìm chính xác label $c$ cho mỗi điểm $\mathbf{x}\in\mathbb{R}^d$, ta có thể **chỉ ra xác suất để điểm đó thuộc label $c$**, tức là $p(y=z\;|\;\mathbf{x})$.

$$
\begin{align}
c&=\arg\max_cp(c|\mathbf{x})\\
&=\arg\max_c\frac{p(\mathbf{x}|c).p(c)}{p(\mathbf{x})}=\arg\max_cp(\mathbf{x}|c).p(c)\tag{1}\\
&=\arg\max_cp(c)\prod_{i=1}^dp(x_i|c)\tag{2}\\
&=\boxed{\arg\max_c\log{p(c)}+\sum_{i=1}^d\log{p(x_i|c)}}\tag{3}
\end{align}
$$

Giải thích một chút:
- $(1)$ có được là nhờ định luật Bayes.
- $(2)$ có được là nhờ giả thiết các biến $x_i\in\mathbf{x}$ độc lập với nhau. Do đó, công thức trên được gọi là *Naive Bayes - Bayes ngây thơ*.
- $(3)$ nhằm hạn chế sai số.

Một số phân phối thường gặp:
1. **Gaussian naive Bayes**: Thường áp dụng cho các đặc trưng có thành phần biến liên tục.
$$p(x_i|c)=p(x_i|\mu_{ci},\sigma^2_{ci})=\frac{1}{\sqrt{2\pi\sigma^2_{ci}}}\exp\left(-\frac{(x_i-\mu_{ci})^2}{2\sigma^2_{ci}}\right)$$

2. **Multinomial naive Bayes**: Thường áp dụng cho những loại vector hóa dựa trên tần suất như BoW.
$$p(x_i|c)=\frac{N_{ci}}{N_c}$$
	Trong đó, $N_{ci}$ là tổng số lần đặc trưng $i$ xuất hiện trong tập class $c$. $N_c$ là kích thước của class $c$.
	Tuy nhiên, nếu $x_i$ là một đặc trưng mới chưa từng xuất hiện trong tập $c$, $p$ dĩ nhiên là 0. Trong trường hợp đó, người ta thường dùng phương pháp **Laplace smoothing**:
$$\hat{p}(x_i|c)=\frac{N_{ci}+\alpha}{N_c+d\alpha};\quad\alpha>0$$
	Thường thì $\alpha=1$.

3. **Bernoulli naive Bayes**: Thường áp dựng với các đặc trưng nhị phân.
$$p(x_i|c)=p(i|c).x_i+(1-p(i|c)).(1-x_i)$$

# Thí nghiệm

Cho set có các loại văn bản sau:

| STT | Nội dung                 | Lớp |
| --- | ------------------------ | --- |
| d1  | hanoi pho chaolong hanoi | B   |
| d2  | hanoi buncha pho omai    | B   |
| d3  | pho banhgio omai         | B   |
| d4  | saigon hutiu banhbo pho  | N   |
| d5  | hanoi hanoi buncha hutiu | ?   |

Mỗi văn bản chỉ thuộc 1 trong 2 lớp: B (Bắc) hoặc N (Nam). **Hãy dựa trên training set (d1 đến d4) để phân lớp cho d5.**

Đối với bài toán này, ta có thể vừa dùng Multinomial hoặc Bernouli. Ta sẽ thử áp dùng Multinomial.

Ta có tần suất xuất hiện các class là:
$$p(B)=3/4;\qquad p(N)=1/4$$
Từ điển:
$$V=\{\text{hanoi, pho, chaolong, buncha, omai, banhgio, saigon, hutiu, banhbo}\}$$

Đối với Multinomial, ta sẽ áp dụng luôn phương pháp Laplace với $\alpha=1$:
$$p(B|d_5)=\frac{1.5\times10^{-4}}{1.5\times10^{-4}+1.75\times10^{-5}}\approx0.8955;\qquad p(N|d_5)\approx0.0145$$
Vậy suy ra $d_5$ có khả năng thuộc vào class B cao hơn.

Dùng Scikit-learn:
```python
import numpy as np

# train data
d1 = [2, 1, 1, 0, 0, 0, 0, 0, 0]
d2 = [1, 1, 0, 1, 1, 0, 0, 0, 0]
d3 = [0, 1, 0, 0, 1, 1, 0, 0, 0]
d4 = [0, 1, 0, 0, 0, 0, 1, 1, 1]
train_data = np.array([d1, d2, d3, d4])
label = np.array(['B', 'B', 'B', 'N'])

# test data
d5 = np.array([[2, 0, 0, 1, 0, 0, 0, 1, 0]])
```

Dùng Multinomial:
```python
from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(train_data, label)

print('Predicting class of d5:', str(model.predict(d5)[0]))
```
```sh
Predicting class of d5: B
```

Dùng Bernoulli:
```python
from sklearn.naive_bayes import BernoulliNB

model = BernoulliNB()
model.fit(train_data, label)
print('Predicting class of d5:', str(model.predict(d5)[0])) # test
```
Kết quả cũng tương tự, dù xác suất có hơi khác một chút.

Bạn có thể kiểm tra xác suất đối với từng class qua cú pháp:
```python
print(model.predict_proba(d5))
```
