
```insta-toc
---
title:
  name: Mục lục
  level: 1
  center: false
exclude: ""
style:
  listType: number
omit: []
levels:
  min: 1
  max: 6
---

# Mục lục

1. Cơ sở Toán học của Logistic regression
2. Thí nghiệm
```

# Cơ sở Toán học của Logistic regression

*Logistic regression* là một dạng nâng cao hơn của [[3.1. Perceptron learning algorithm (PLA)|PLA]], giải quyết được các trường hợp non linearly separable. Ý tưởng của nó cũng tương tự như PLA, là tìm một boundary phân chia 2 class.

Ví dụ: Cho các điểm sau: (*Mối quan hệ giữa việc học và thi đỗ / trượt*):
![](https://machinelearningcoban.com/assets/LogisticRegression/ex1.png)

Có những boundary nào có thể phân chia được class này?
![](https://machinelearningcoban.com/assets/LogisticRegression/activation.png)
- **Linear**: Không ổn, vì hàm số không bị chặn trong khoảng (0, 1).
- **Hard threshold**: Là một dạng sign trong [[3.1. Perceptron learning algorithm (PLA)|PLA]], vì dữ liệu không phải linearly separable.
- **Các hàm sigmoid**: Phù hợp hơn với bài toán, vì:
	- Là hàm liên tục, bị chặn trong khoảng (0, 1).
	- Hình dáng đồ thị phù hợp với logic thực tế, càng học ít thì càng dễ fail, và ngược lại.
	- Mượt, có đạo hàm ở mọi nơi, thuận tiện cho tối ưu.

Có 2 dạng hàm sigmoid thường dùng:
$$
z_i=f(\mathbf{w}^T\mathbf{x}_i)=f(s)=
\left[
\begin{align}
&\frac{1}{1+s^{-s}}\\
&\frac{e^s}{e^s+s^{-s}}\\
\end{align}
\right.
$$

![](https://machinelearningcoban.com/assets/LogisticRegression/lg_results.png)

Đối với logistic regression thì đầu ra được biểu diễn dưới dạng xác suất một điểm vào các class.
$$\boxed{\mathbf{w}=\arg\max_\mathbf{w}P(\mathbf{y}|\mathbf{X};\mathbf{w})}$$
Trong đó, **hàm mất mát** chính là xác suất của mỗi điểm vào các class, tức là phân phối xác suất:
$$
\begin{align}
P(\mathbf{y}|\mathbf{X};\mathbf{w})&=\prod_{i=1}^NP(y_i|\mathbf{x_i};\mathbf{w})\\
&=\prod_{i=1}^Nz_i^{y_i}(1-z_i)^{1-y_i}\\
\end{align}
$$
- Khi $y_i=1$, xác suất để điểm $\mathbf{x}_i$ rơi vào class $y_i$ là $P(y_i=1|\mathbf{x}_i;\mathbf{w})=f(\mathbf{w}^T\mathbf{x}_i)$.
- Ngược lại là: $P(y_i=0|\mathbf{x}_i;\mathbf{w})=1-f(\mathbf{w}^T\mathbf{x}_i)$.

Như vậy:
$$
\begin{align}
J(\mathbf{w};\mathbf{x},\mathbf{y})&=-\log{P(\mathbf{y}|\mathbf{X};\mathbf{w})}\\
&=\boxed{-\sum_{i=1}^N(y_i\log{z_i}+(1-y_i)\log{(1-z_i)})}
\end{align}
$$

Hàm này được gọi là *Cross-entropy*, tìm hiểu sâu hơn [[3.3. Softmax regression]].

Thuật toán này cũng tối ưu $\mathbf{w}$ bằng GD như PLA:
$$
\boxed{\mathbf{w}\leftarrow\mathbf{w}+\eta.(y_i-z_i).\mathbf{x}_i}
$$

# Thí nghiệm
