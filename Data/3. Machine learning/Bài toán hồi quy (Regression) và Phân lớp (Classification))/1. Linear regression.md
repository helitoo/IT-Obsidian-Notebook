
```insta-toc
---
title:
  name: Mục lục
  level: 1
  center: false
exclude: ""
style:
  listType: number
omit: []
levels:
  min: 1
  max: 6
---

# Mục lục

1. Bài toán linear regression
2. Cơ sở Toán học của linear regression
    1. Hàm tuyến tính đi qua gốc tọa độ
    2. Hàm tuyến tính có hệ số tự do (Affine)
3. Thí nghiệm
4. Hạn chế của OLS regession
5. Các phương pháp tối ưu linear regression
    1. Ridge regression
    2. Huber regression
```


# Bài toán linear regression

**Linear regression (Linear fitting, Linear least square, Hồi quy tuyến tính)** là một dạng [[1. Các khái niệm cơ bản trong machine learning#Regression và Completion|Regression]]. Giả sử các vector sẵn có và vector đầu vào tuân theo một hàm tuyến tính, nhiệm vụ của bài toán là tìm trọng số tối ưu của hàm tuyến tính đó để xây dựng một hàm hoàn chỉnh.

# Cơ sở Toán học của linear regression

## Hàm tuyến tính đi qua gốc tọa độ

Gọi:
- Các vector sẵn có $\mathbf{X}\in\mathbb{R}^{d\times N}$.
- Vector các trọng số $\mathbf{w}\in\mathbb{R}^d$.
Tức là $x_{ij}\in\mathbf{x}_i\in\mathbf{X}$ sẽ có trọng số là $w_j$.

Khi đó, hàm tuyến tính sẽ là:
$$\boxed{\mathbf{Y}\approx\mathbf{\hat{Y}}=\mathbf{Xw}};\quad
\mathbf{y}_i\approx\mathbf{\hat{y}}_i=\mathbf{x}^T_i\mathbf{w}=
w_1x_{i_1}+w_2x_{i_2}+...+x_dx_{i_d}$$

**Hàm mất mát**:
$$\begin{align}
\mathcal{L}(\mathbf{w})
&=\frac{1}{2N}\sum_{i=1}^N (y_i-\hat{y_i})^2\\
&=\frac{1}{2N}\sum_{i=1}^N (y_i-\mathbf{x}_i^T\mathbf{w})^2\\
&=\boxed{\frac{1}{2N}\|y - X^T\mathbf{w}\|_2^2}\\
\end{align}
$$
Trong đó:
- Việc lấy bình phương là để đảm bảo sai số không ẩm.
- Việc chia cho 2 chỉ để thuận tiện cho việc đạo hàm tìm nghiệm.

**Nghiệm của bài toán tìm tham số tối ưu**: Chính là $w$ sao cho:
$$\mathbf{w}=\arg\max_\mathbf{w}\mathcal{L}(\mathbf{w})$$
Để giải phương trình trên, ta cần giải phương trình đạo hàm của hàm này:
$$
\frac{\nabla\mathcal{L}(\mathbf{w})}{\nabla\mathbf{w}}
=\frac{1}{N}X^T(X\mathbf{w}-\mathbf{y})=0\Leftrightarrow X^TX\mathbf{w}=X^T\mathbf{y}
$$
- Nếu $X^TX$ khả nghịch, phương trình có nghiệm duy nhất $\boxed{\mathbf{w}=(X^TX)^{-1}X^T\mathbf{y}}$.
- Ngược lại, phương trình vô nghiệm hoặc có vô số nghiệm tùy ý. Tuy nhiên, do mọi ma trận $X^TX$ đều có ma trận giả nghịch đảo $(X^TX)^+$ tương ứng, nên phương trình vẫn có nghiệm đặc biệt là $\boxed{\mathbf{w}=(X^TX)^{+}X^T\mathbf{y}}$.

## Hàm tuyến tính có hệ số tự do (Affine)

Do là một hàm tuyến tính, $\hat{y_i}$ cũng có thể có hệ số tự do:
$$\hat{y_i}=\mathbf{x}_i\mathbf{w}+b$$
- Mối quan hệ trên được gọi là **Affine**.
- Hệ số $b$ được gọi là **Hệ số tự do (Bias)**.

**Bias trick** là kỹ thuật lồng bias vào vector tự do: Ta coi $b$ thực chất là trọng số của $x_0=1$.
- $\overline{X}=\{\overline{\mathbf{x}_1};...;\overline{\mathbf{x}_N}\}$, $\overline{\mathbf{x}_i}=[1;x_1;...;x_n]$.
- $\overline{\mathbf{w}}=\{b;w_1;...;w_n\}$.
$$\mathbf{y}\approx\hat{\mathbf{y}}=\overline{X}.\overline{\mathbf{w}}$$

Lúc này, $\mathbf{w}$ sẽ là:
$$\boxed{\overline{\mathbf{w}}=\left(\overline{X}^T\overline{X}\right)^+\overline{X}\mathbf{y}}$$

# Thí nghiệm

Hãy sử dụng bảng dữ liệu sau làm dữ liệu training cho bài toán ước lượng xem một người có cân nặng 155 sẽ có chiều cao là bao nhiêu?

| Chiều cao (cm) | Cân nặng (kg) | Chiều cao (cm) | Cân nặng (kg) |
| -------------- | ------------- | -------------- | ------------- |
| 147            | 49            | 168            | 60            |
| 150            | 50            | 170            | 72            |
| 153            | 51            | 173            | 63            |
| 155            | 52            | 175            | 64            |
| 158            | 54            | 178            | 66            |
| 160            | 56            | 180            | 67            |
| 163            | 58            | 183            | 68            |
| 165            | 59            | --             | --            |

```python
import numpy as np

# height
X = np.array([[147, 150, 153, 158, 163, 165, 168, 170, 173, 175, 178, 180, 183]]).T

# weight
y = np.array([ 49, 50, 51, 54, 58, 59, 60, 62, 63, 64, 66, 67, 68])

# value to be estimated
x = 155
```

Nghiệm hồi quy tuyến tính: $y\approx w_0x+w_1$.
Nghiệm tham số tối ưu: $\mathbf{\overline{w}}=(\overline{X}^T.\overline{X})^+\overline{X}^T\mathbf{y}$.

```python
# Building Xbar (\overline{x})
one = np.ones((X.shape[0], 1))            # 
Xbar = np.concatenate((one, X), axis = 1)

# Calc w
A = np.dot(Xbar.T, Xbar)         # Xbar^T . XBar
b = np.dot(Xbar.T, y)            # Xbar^T . y
w = np.dot(np.linalg.pinv(A), b) # (Xbar^T . XBar)^+ * Xbar^T . y

# Final result
y1 = w[0] * 155 + w[1]
```

Kết quả là `52.94`.

Module `scikit-learn` cũng có kết quả giống hệt
```python
from sklearn import datasets, linear_model

regr = linear_model.LinearRegression()
regr.fit(X, y)

y1 = regr.coef_[0] * 155 + regr.coef_[0]
```

# Hạn chế của OLS regession

1. **Nhạy cảm với nhiễu (Sensitive to outlier)**: Chỉ cần một hay vì điểm dữ liệu quá lệch so với các dữ liệu còn lại cũng tạo ra đáp án khác biệt. Có 2 cách thông dụng để khắc phục:
	1. Loại bỏ nhiễu trước khi đưa vào mô hình.
	2. Sử dụng hàm mất mát **Huber loss**. Linear regression sử dụng Hober loss gọi là Huber regession.

2. **Không áp dụng được với các quan hệ phi tuyến**.

# Các phương pháp tối ưu linear regression

## Ridge regression

>[!NOTE]
>**Ridge regression** là sự tối ưu của của **OLS**, giúp:
>1. Đảm bảo luôn tồn tại nghiệm tối ưu $\mathbf{w}$.
>2. Giảm tác động của các outlier thông qua hệ số ước lượng $\lambda$.

Xét ma trận $X^TX$ bậc $d$ là ma trận nửa xác định dương, bởi vì:
$$\forall \mathbf{v} \in \mathbb{R}^d;\quad
\mathbf{v}^TX^TX\mathbf{v}=(X\mathbf{v})^TX\mathbf{v}=\|X\mathbf{v}\|_2^2\geq0
$$
Gọi $\lambda I_d$ là ma trận vuông có các đường chéo là $\lambda$. Khi đó ma trận $A^TA+\lambda I$ sẽ **luôn khả nghịch** vì các trị riêng đã được cộng thêm một khoảng là $\lambda$, $\lambda$ được gọi là **hệ số ước lượng**.

**Nghiệm của bài toán**:
$$\mathbf{y}=(X^TX+\lambda I)^{-1}X\mathbf{y}$$

**Hàm mất mát Ridge**:
$$\mathcal{L}_2(\mathbf{w})=\frac{1}{2N}(\|\mathbf{y}-X^T\mathbf{w}\|_2^2+\lambda\|\mathbf{w}\|_2^2)$$
Có thể thấy Ridge loss là một [[2.2 Overfitting#Regularization|Regularization loss function]] sử dụng norm $l_2$.

**Nghiệm tham số tối ưu**:
$$\frac{\nabla\mathcal{L}(\mathbf{w})}{\nabla\mathbf{w}}
=\frac{1}{N}(X(X^T\mathbf{w}-\mathbf{y})+\lambda\mathbf{w})=0
\Leftrightarrow(XX^T+\lambda I)\mathbf{w}=X\mathbf{y}
\Rightarrow\boxed{\mathbf{w}=(X^TX+\lambda I)^{-1}X\mathbf{y}}
$$

## Huber regression










