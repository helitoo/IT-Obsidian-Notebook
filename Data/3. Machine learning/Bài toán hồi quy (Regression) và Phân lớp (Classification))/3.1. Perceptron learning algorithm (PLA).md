
```insta-toc
---
title:
  name: Mục lục
  level: 1
  center: false
exclude: ""
style:
  listType: number
omit: []
levels:
  min: 1
  max: 6
---

# Mục lục

1. Cơ sở Toán học của Perceptron learning
2. Thí nghiệm
```

# Cơ sở Toán học của Perceptron learning

*Perceptron* là một thuật toán sơ khai giải quyết bài toán classification ở mức độ nhị phân, tức là chỉ có 2 class. Perceptron có thể chỉ ra 1 ranh giới tuyến tính (*boundary*) giữa 2 class đó.

![](https://machinelearningcoban.com/assets/pla/pla2.png)

Gọi:
- $\mathbf{X}=[\mathbf{x}_1,\mathbf{x}_2,...,\mathbf{x}_N]\in\mathbb{R}^{d\times N}$ là tập các điểm dữ liệu cần xét.
- $\mathbf{y}=[y_1,y_2,...,y_N]\in\mathbb{R}^{1\times N}$ là tập các label, $y_i=1$ nếu $\mathbf{x}_i$ thuộc class thứ nhất, và $-1$ với class kia.

**Phương trình boundary**:
$$
\begin{align}
f_\mathbf{w}(\mathbf{x})&=w_1x_1+...+w_dx_d+w_0\\
&=\boxed{\mathbf{w}^T\mathbf{x}=0}
\end{align}
$$

Trong đó, $\mathbf{x}$ là affine đã được trình bày trong [[1. Linear regression]].

Khi đó, nếu $\mathbf{w}^T\mathbf{x}\geq0$ thì điểm $\mathbf{x}$ được gọi là thuộc class thứ nhất, ngược lại là class kia. Ký hiệu:
$$\text{label}(\mathbf{x})=\text{sgn}(\mathbf{w}^T\mathbf{x})$$

![](https://machinelearningcoban.com/assets/pla/pla4.png)

**Hàm mất mát**: Là dựa trên số lượng các điểm bị phân loại sai (*misclassified*).

$$
\begin{align}
J(\mathbf{w})&\approx\sum_{\mathbf{x}_i\in\mathcal{M}}(-y_i.\text{sign}(\mathbf{w}^T\mathbf{x}_i))\tag{1}\\
&=\boxed{\sum_{\mathbf{x}_i\in\mathcal{M}}(-y_i\mathbf{w}^T\mathbf{x}_i)}\tag{2}
\end{align}
$$
Trong đó:
- $\mathcal{M}$ là tập hợp các điểm bị misclassified.
- Ở $(1)$, do $\mathbf{x}_i$ là misclassified nên $y_i=-\text{sign}(\mathbf{w}^T\mathbf{x}_i)$, và do đó $-y_i.\text{sign}(\mathbf{w}^T\mathbf{x}_i)=1$.
- Ở $(2)$, ta có nhận xét nếu một điểm bị misclassified càng xa boudary thì $-y_i\mathbf{w}^T\mathbf{x}_i$ càng lớn, nên độ mất mát cũng càng lớn theo. Phiên bản $(1)$ không thể hiện được điều này.

Phương pháp tìm $\mathbf{w}$ ở đây là GD với công thức cập nhật là:
$$\boxed{\mathbf{w}\leftarrow\mathbf{w}+\eta.y_i.\mathbf{x}_i}$$

Khi $\mathbf{w}$ càng tiến gần đến nghiệm tối ưu thì độ mất mát càng nhỏ, các điểm được classify càng đúng. Nên cách hoạt động của PLA là chọn ngẫu nhiên một boundary, sau đó hiệu chỉnh $\mathbf{w}$ cho đến khi đạt đến nghiệm tối ưu.

**Chú ý**:
1. PLA có thể cho ra nhiều boundary khác nhau cùng thỏa mãn bài toán.
2. Dữ liệu $\mathbf{x}$ đầu vào phải có thể được phân chia bởi boundary (*linearly separable*). Nếu có nhiều điểm dữ liệu nhiễu, PLA vẫn có thể cho ra một nghiệm khá tối ưu, tuy nhiên ta có cách hay hơn là dùng **Pocket algorithm**.

# Thí nghiệm

Phân lớp 2 tập điểm sau:
```python
import numpy as np 
from scipy.spatial.distance import cdist
np.random.seed(2)

means = [[2, 2], [4, 2]]
cov = [[.3, .2], [.2, .3]]

N = 10
X0 = np.random.multivariate_normal(means[0], cov, N).T
X1 = np.random.multivariate_normal(means[1], cov, N).T

X = np.concatenate((X0, X1), axis = 1)
X = np.concatenate((np.ones((1, 2*N)), X), axis = 0)

# Đáp án
y = np.concatenate((np.ones((1, N)), -1*np.ones((1, N))), axis = 1)
```

Thuật toán PLA:
```python
# Tính label
def h(w, x):    
    return np.sign(np.dot(w.T, x))

# Kiểm tra w đã hội tụ chưa
def has_converged(X, y, w):    
    return np.array_equal(h(w, X), y) 

# PLA
def perceptron(X, y, w_init):
    w = [w_init]
    N = X.shape[1]
    d = X.shape[0]
    mis_points = []
    
    while True:
        # mix data 
        mix_id = np.random.permutation(N)
        
        for i in range(N):
            xi = X[:, mix_id[i]].reshape(d, 1)
            yi = y[0, mix_id[i]]
            if h(w[-1], xi)[0] != yi:
                mis_points.append(mix_id[i])
                w_new = w[-1] + yi*xi 
                w.append(w_new)
                
        if has_converged(X, y, w[-1]):
            break
    
    return (w, mis_points)

d = X.shape[0]
w_init = np.random.randn(d, 1)
(w, m) = perceptron(X, y, w_init)
```

![](https://machinelearningcoban.com/assets/pla/pla_vis.gif)
