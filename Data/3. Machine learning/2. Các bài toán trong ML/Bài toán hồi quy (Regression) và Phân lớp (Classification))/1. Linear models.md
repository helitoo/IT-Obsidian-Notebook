
**Tài liệu tham khảo**:
1. https://scikit-learn.org/stable/modules/linear_model.html.
2. https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge.html.
3. https://scikit-learn.org/stable/modules/model_evaluation.html.

```insta-toc
---
title:
  name: Mục lục
  level: 1
  center: false
exclude: ""
style:
  listType: number
omit: []
levels:
  min: 1
  max: 6
---

# Mục lục

1. Bài toán linear regression
2. Một số dạng linear models
    1. Ordinary least squares (OLS)
    2. Ridge regression
    3. Logistic regression
```

# Bài toán linear regression

**Linear regression (Linear fitting, Linear least square, Hồi quy tuyến tính)** là một dạng Regression (hồi quy). Giả sử các vector sẵn có và vector đầu vào tuân theo một *hàm tuyến tính*, nhiệm vụ của bài toán là tìm trọng số tối ưu của hàm tuyến tính đó để xây dựng một hàm hoàn chỉnh.

Gọi:
- $\hat{y}$: Giá trị cần dữ đoán.
- $w\in\mathbb{R}^p$: Vector trọng số (`coef_`). Trong đó thì $w_0$ là trọng số tự do (`intercept_`).

Công thức model:
$$\boxed{\hat{y}(w,X)=wX=w_0+w_1x_1+...+w_px_p}$$


# Một số dạng linear models

## Ordinary least squares (OLS)

OLS là dạng cơ bản nhất của linear models với ý tưởng bám sát những gì đã nói bên trên.

```python
from sklearn import linear_model
reg = linear_model.LinearRegression()

reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
```

Hàm mất mát của OLS là **MSE (Mean squared error)**:
- Là sai số trung bình giữa giá trị dự đoán $\hat{y}$ (`y_pred`) và giá trị kỳ vọng $y$ (`y_true`).
- **Cú pháp**: `sklearn.metrics.mean_squared_error(y_true, y_pred)`.
- **Công thức**:$$
 \begin{align}
 \mathcal{L}_1(y,\hat{y})&=\frac{1}{|\textbf{X}|}\|y-\hat{y}\|_2^2\\
 &=\frac{1}{|\textbf{X}|}\sum(y_i-\hat{y}_i)^2
 \end{align}
 $$

**Nhược điểm**:
1. **Nhạy cảm với nhiễu (Sensitive to outlier)**: Chỉ cần một hay vì điểm dữ liệu quá lệch so với các dữ liệu còn lại cũng tạo ra đáp án khác biệt.
2. **Không áp dụng được với các quan hệ phi tuyến**.

## Ridge regression

Ridge là một cải tiến của OLS, nó "trơ" với các noises. Để làm được điều này, Ridge thêm một hệ số phạt $\alpha\|w\|^2_2$ (**penalty**) vào hàm mất mát.

$$\mathcal{L}_2(y,\hat{y})=\frac{1}{|X|}\|y-\hat{y}\|_2^2+\alpha\|w\|^2_2\;;\quad\alpha>0$$

$\alpha$ càng lớn, hệ số phạt càng cao, $w$ càng lớn. Mục tiêu của model là tối ưu hàm mất mát nên không thể chọn các điểm có hệ số phạt quá cao.

| ![](https://scikit-learn.org/stable/_images/sphx_glr_plot_ols_ridge_002.png) | ![](https://scikit-learn.org/stable/_images/sphx_glr_plot_ols_ridge_003.png) |
| ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |

```python
from sklearn import linear_model
reg = linear_model.Ridge(alpha=.5)
reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
```

## Logistic regression

**Logistic regression** (còn gọi là *logit regression, maximun-entropy classification, log-linear classifier,...*) thường được dùng cho bài toán phân loại (classification) hơn là hồi quy, nhưng bản chất nó là hồi quy.

Ý tưởng của logistic là xây dựng một hàm **logistic function**. Logistic function là một dạng **sigmoid curve**, tức là nó có hình dạng như một đường cong chữ S chia đôi miền giá trị theo chiều dọc.

**Binary logistic**:
- Là trường hợp đặc biệt của logistic, trong đó logistic function bị chặn trong đoạn $[0;1]$. Và các điểm dữ liệu $y\in\{0;1\}$.
- Với mỗi điểm dữ liệu, thuật toán sẽ kiểm tra xác suất để $\hat{y}=1$ hay $\hat{y}=-1$. Tức là: $$
\hat{y}=
\begin{cases}
1&\quad P(y=1|x)\geq0.5\\
0&\quad\text{otherwise}
\end{cases}
$$
- Hàm mất mát của binary logistic là **`sklearn.metrics.log_loss(y_true, y_pred)` (Negative log-likelihood)**: $$\mathcal{L}_\text{log}(y,p)=\frac{1}{|X|}\sum(-y\log(p)+(1-y)\log(1-p))+\alpha.\Omega(w)$$, với:
	- $p=P(y=1|x)$.
	- $\alpha=\dfrac{1}{c}$: Hệ số phạt.
	- $\Omega(w)$: Hàm phạt (Penalty).
- Cú pháp: `sklearn.linear_model.LogisticRegression(penalty='deprecated', C=1.0)`.
- Một số hàm phạt:
	- `None`: $0$.
	- `"l1"`: $\|w\|_1$.
	- `"l2"`: $\dfrac{1}{2}\|w\|^2_2$.
	- `"elasticnet"`: $\alpha\|w\|_1+\dfrac{1-\alpha}{2}\|w\|^2_2$.

Trường hợp tổng quát của Binary logistic là **Multinomial logistic** sẽ được tìm hiểu sau, vì có rất nhiều cách triển khai Multinomial logistic.




















